<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Samir Lamara on Samir Lamara</title>
    <link>https://slamara.github.io/</link>
    <description>Recent content in Samir Lamara on Samir Lamara</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Tuning XGBoost using grid search results in R</title>
      <link>https://slamara.github.io/post/xgboost/</link>
      <pubDate>Thu, 09 Aug 2018 13:54:32 +0200</pubDate>
      
      <guid>https://slamara.github.io/post/xgboost/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Titanic: Machine Learning from Disaster- A Kaggle Competition</title>
      <link>https://slamara.github.io/project/titanic/</link>
      <pubDate>Wed, 08 Aug 2018 17:42:32 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/titanic/</guid>
      <description>&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;The Titanic left the port of Queensland (now Cobh), Ireland on April 11, 1912 with 2224 passengers &lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. On early morning of Monday, 15 April 1912, sank the “unsinkable” after a collision with an iceberg dragging more than 2/3 of the passengers down to the bottom of the Atlantic Ocean.&lt;/p&gt;
&lt;p&gt;Due to the insufficient number of means of survival, the possibility to get in lifeboats would have obeyed to some kind of considerations: from the ethical “women and children first” to the more practical proximity to the Upper Deck which is itself related to the cabin class and thus to the socio-economic class of each passenger (see &lt;a href=&#34;https://rmstitanic1912.weebly.com/the-levels-of-the-titanic.html&#34;&gt;here&lt;/a&gt; the different levels of the Titanic).&lt;/p&gt;
&lt;p&gt;Based on these considerations, the purpose of this project is to predict the survival of a test subset of passengers using machine learning. To this intent, I “dive” first into the training dataset provided by &lt;a href=&#34;https://www.kaggle.com/c/titanic&#34;&gt;kaggle.com&lt;/a&gt; to identify and engineer the most important predictors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;read-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Read data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(Amelia)
library(gridExtra)
library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trainVarTypes &amp;lt;- c(&amp;quot;integer&amp;quot;, &amp;quot;factor&amp;quot;, &amp;quot;factor&amp;quot;, &amp;quot;character&amp;quot;, &amp;quot;factor&amp;quot;, &amp;quot;numeric&amp;quot;, 
                   &amp;quot;integer&amp;quot;, &amp;quot;integer&amp;quot;, &amp;quot;character&amp;quot;, &amp;quot;numeric&amp;quot;, &amp;quot;character&amp;quot;, &amp;quot;factor&amp;quot;) 
# for Resp. PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked

testVarTypes &amp;lt;- trainVarTypes[-2]

train &amp;lt;- read.csv(&amp;quot;./titanic/train.csv&amp;quot;, colClasses = trainVarTypes, na.strings = c(&amp;quot;NA&amp;quot;, &amp;quot;&amp;quot;))
test &amp;lt;- read.csv(&amp;quot;./titanic/test.csv&amp;quot;, colClasses = testVarTypes, na.strings = c(&amp;quot;NA&amp;quot;, &amp;quot;&amp;quot;))

train$Survived &amp;lt;- factor(train$Survived, levels = c(0,1), labels = c(&amp;quot;no&amp;quot;, &amp;quot;yes&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;preliminary-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preliminary data analysis&lt;/h3&gt;
&lt;p&gt;The train dataset consists in a subset of 891 observations of 11 variables describing each passenger listed in. According to each variable we can know: whether the passenger survived or not (yes/no), his/her class on board (1st, 2nd or 3rd class), name (including the title), sex (female/male), Age, ticket number, fare, cabin number, and where he/her embarked.&lt;/p&gt;
&lt;p&gt;Adding to that, the variables SibSp and Parch represent respectively the number of siblings and spouses, and the number of parents and children. Both variables indicate implicitly whether a passenger has family members on board and how many are they. (more details can be found &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    891 obs. of  12 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Survived   : Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 1 2 2 2 1 1 1 1 2 2 ...
##  $ Pclass     : Factor w/ 3 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;: 3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : chr  &amp;quot;Braund, Mr. Owen Harris&amp;quot; &amp;quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&amp;quot; &amp;quot;Heikkinen, Miss. Laina&amp;quot; &amp;quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&amp;quot; ...
##  $ Sex        : Factor w/ 2 levels &amp;quot;female&amp;quot;,&amp;quot;male&amp;quot;: 2 1 1 1 2 2 2 2 1 1 ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : chr  &amp;quot;A/5 21171&amp;quot; &amp;quot;PC 17599&amp;quot; &amp;quot;STON/O2. 3101282&amp;quot; &amp;quot;113803&amp;quot; ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : chr  NA &amp;quot;C85&amp;quot; NA &amp;quot;C123&amp;quot; ...
##  $ Embarked   : Factor w/ 3 levels &amp;quot;C&amp;quot;,&amp;quot;Q&amp;quot;,&amp;quot;S&amp;quot;: 3 1 3 3 3 2 3 3 3 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(train, n = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PassengerId Survived Pclass
## 1           1       no      3
## 2           2      yes      1
## 3           3      yes      3
## 4           4      yes      1
##                                                  Name    Sex Age SibSp
## 1                             Braund, Mr. Owen Harris   male  22     1
## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1
## 3                              Heikkinen, Miss. Laina female  26     0
## 4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1
##   Parch           Ticket    Fare Cabin Embarked
## 1     0        A/5 21171  7.2500  &amp;lt;NA&amp;gt;        S
## 2     0         PC 17599 71.2833   C85        C
## 3     0 STON/O2. 3101282  7.9250  &amp;lt;NA&amp;gt;        S
## 4     0           113803 53.1000  C123        S&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PassengerId    Survived  Pclass      Name               Sex     
##  Min.   :  1.0   no :549   1:216   Length:891         female:314  
##  1st Qu.:223.5   yes:342   2:184   Class :character   male  :577  
##  Median :446.0             3:491   Mode  :character               
##  Mean   :446.0                                                    
##  3rd Qu.:668.5                                                    
##  Max.   :891.0                                                    
##                                                                   
##       Age            SibSp           Parch           Ticket         
##  Min.   : 0.42   Min.   :0.000   Min.   :0.0000   Length:891        
##  1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000   Class :character  
##  Median :28.00   Median :0.000   Median :0.0000   Mode  :character  
##  Mean   :29.70   Mean   :0.523   Mean   :0.3816                     
##  3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000                     
##  Max.   :80.00   Max.   :8.000   Max.   :6.0000                     
##  NA&amp;#39;s   :177                                                        
##       Fare           Cabin           Embarked  
##  Min.   :  0.00   Length:891         C   :168  
##  1st Qu.:  7.91   Class :character   Q   : 77  
##  Median : 14.45   Mode  :character   S   :644  
##  Mean   : 32.20                      NA&amp;#39;s:  2  
##  3rd Qu.: 31.00                                
##  Max.   :512.33                                
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One important aspect when predicting using machine learning is the assessment of the completeness of the data (percentage of data with one or more values).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;missmap(train, main = &amp;quot;Missing Values Analysis&amp;quot;, col=c(&amp;quot;red&amp;quot;, &amp;quot;gray&amp;quot;)) # how many observations are missing&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/missing-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sapply(train, function(x) sum(is.na(x))) # how many observations are missing&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## PassengerId    Survived      Pclass        Name         Sex         Age 
##           0           0           0           0           0         177 
##       SibSp       Parch      Ticket        Fare       Cabin    Embarked 
##           0           0           0           0         687           2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The analysis shows that two variables suffer strongly from lack of data: Cabin and Age.&lt;/p&gt;
&lt;p&gt;While a missing Cabin number simply means that the passenger has no cabin, the variable Age is crucial and its missing observations should be imputed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-engineering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Engineering&lt;/h3&gt;
&lt;div id=&#34;replace-titles-in-names&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Replace titles in names&lt;/h4&gt;
&lt;p&gt;The preliminary data analysis puts light on the fact that the passengers titles are included to the variable Names. While the passengers names are not of a big importance here, their titles are in contrast very important and could represent an important predictor for machine learning.&lt;/p&gt;
&lt;p&gt;I chose thus to isolate the titles and classify them in four different categories according to their meanings: “Miss” for young ladies, “Mrs” for older/married ones, “Mil” for military passengers and “Noble” for the aristocratic titles. The categories “Rev”, “Mr”, “Dr” and “Master” are kept unchangeable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1. train$Name - Replace titles in Name

x &amp;lt;- sub(&amp;quot;.*, &amp;quot;, &amp;quot;&amp;quot;, as.character(train$Name))
train$NameCat &amp;lt;- sub(&amp;quot;\\..*&amp;quot;, &amp;quot;&amp;quot;, x)
train$NameCat[train$NameCat %in% c(&amp;quot;Miss&amp;quot;, &amp;quot;Mlle&amp;quot;, &amp;quot;Ms&amp;quot;)] &amp;lt;- &amp;quot;Miss&amp;quot;
train$NameCat[train$NameCat %in% c(&amp;quot;Mme&amp;quot;, &amp;quot;Mrs&amp;quot;)] &amp;lt;- &amp;quot;Mrs&amp;quot;
train$NameCat[train$NameCat %in% c(&amp;quot;Capt&amp;quot;, &amp;quot;Col&amp;quot;, &amp;quot;Major&amp;quot;)] &amp;lt;- &amp;quot;Mil&amp;quot;
train$NameCat[train$NameCat %in% c(&amp;quot;Don&amp;quot;, &amp;quot;Jonkheer&amp;quot;, &amp;quot;Lady&amp;quot;, &amp;quot;Sir&amp;quot;, &amp;quot;the Countess&amp;quot;)] &amp;lt;- &amp;quot;Noble&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note however, that, contrary to what one might think, the category “Master” was used at the time to identify male children.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(train, aes(x = NameCat, y = Age)) + geom_boxplot(aes(fill=Survived)) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/bxplt_NameCat-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-age-categories&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Create Age categories&lt;/h4&gt;
&lt;p&gt;For efficiency, it is more appropriate to classify the passengers according to specific age ranges using the variable Age. In this way, one could directly identify whether the passenger is a “child”, “adult” or “senior”.&lt;/p&gt;
&lt;p&gt;For passengers with no information about the age, I use temporarily the category “missing”.&lt;/p&gt;
&lt;p&gt;Please note also, that eventhough the Titanic’s Certificates of Clearance indicates that passengers over 14 are considered adults &lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, I consider myself that during the evacuation of the Titanic, only signs of puberty can be used to determine whether a passenger is a child or not. For this reason, I chose the age of 14 as a limit to distinguish between an adult or a child passenger.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 2. train$Age - Categories

train$AgeCat[train$Age &amp;gt; 0 &amp;amp; train$Age &amp;lt;= 14 &amp;amp; !is.na(train$Age)] &amp;lt;- &amp;quot;child&amp;quot;
train$AgeCat[train$Age &amp;gt; 14 &amp;amp; train$Age &amp;lt;= 60 &amp;amp; !is.na(train$Age)] &amp;lt;- &amp;quot;adult&amp;quot;
train$AgeCat[train$Age &amp;gt; 60 &amp;amp; !is.na(train$Age)] &amp;lt;- &amp;quot;senior&amp;quot;
train$AgeCat[is.na(train$Age)] &amp;lt;- &amp;quot;missing&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cabin-number&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Cabin number&lt;/h4&gt;
&lt;p&gt;A new categorical variable is also set according to whether the passenger has a cabin or not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 3. train$Cabin

train$HasCab[!is.na(train$Cabin)] &amp;lt;- &amp;quot;yes&amp;quot;
train$HasCab[is.na(train$Cabin)] &amp;lt;- &amp;quot;no&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-family-factor&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The “family” factor&lt;/h4&gt;
&lt;p&gt;It is known that many passengers were crossing the Atlantic with other members of their families. On living the sinking Titanic, it is obvious that children should have been accompanied on lifeboats by, at least, one of their parents. Thus, being on board with other family members would play an important role.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 4. train$HasFam
train$HasFam[train$SibSp != 0 | train$Parch != 0] &amp;lt;- &amp;quot;yes&amp;quot;
train$HasFam[train$SibSp == 0 &amp;amp; train$Parch == 0] &amp;lt;- &amp;quot;no&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nevertheless, the question is: to what extent? did big families have the same chance of getting in lifeboats as smaller ones?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Check the relationship between SibSp, Parchar and survival

ggplot(train, aes(y = SibSp, x = Parch)) +
    geom_jitter(aes(color = Survived)) +
    theme_bw() +
    geom_vline(xintercept = 3, color = &amp;#39;black&amp;#39;, lty=5) +
    geom_hline(yintercept = 3, color = &amp;#39;black&amp;#39;, lty=5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It appears here that passengers with SibSp or Parch greater than 3 (i.e. the passenger belonged to a family of more than 4 members) had a smaller chance to survive. I introduce thus another predictor variable which classify the passengers according to the count of their relatives on board.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 5. train$FamCat

train$FamCat[train$SibSp + train$Parch &amp;lt;= 4] &amp;lt;- &amp;quot;small&amp;quot;     # family
train$FamCat[train$SibSp + train$Parch &amp;gt; 4] &amp;lt;- &amp;quot;big&amp;quot;        # big one
train$FamCat[train$SibSp + train$Parch == 0] &amp;lt;- &amp;quot;single&amp;quot;    # the passenger was alone&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;factorize-categorical-variables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Factorize categorical variables…&lt;/h4&gt;
&lt;p&gt;…as all these engineered variables are to be used as categorical ones:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train$Pclass &amp;lt;- factor(train$Pclass)
train$NameCat &amp;lt;- factor(train$NameCat)
train$AgeCat &amp;lt;- factor(train$AgeCat)
train$HasCab &amp;lt;- factor(train$HasCab)
train$HasFam &amp;lt;- factor(train$HasFam)
train$FamCat &amp;lt;- factor(train$FamCat)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;detailed-exploratory-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Detailed exploratory data analysis&lt;/h3&gt;
&lt;p&gt;According to the train dataset, the proportion of survival is a bit greater than the global one (one third). Not surprisingly, the women and children survived the shipwreck more than men.&lt;/p&gt;
&lt;p&gt;One can also notice that the overall proportion of survival for the category “missing” seems to be very close to category “adult” which leads me to believe (even subjectively) that the mean ages of these categories would not be very different.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# barplot: survived or not
p1 &amp;lt;- ggplot(data = train, aes(Survived, fill = Survived)) + 
    geom_bar() + guides(fill = FALSE) + 
    theme_bw()

# barplot: survival according to sex
p2 &amp;lt;-  ggplot(data = train, aes(Sex, fill = Survived)) + 
    geom_bar(position = &amp;quot;dodge&amp;quot;) + guides(fill = FALSE) + 
    theme_bw()

# barplot: survival or not according to age category
p3 &amp;lt;-  ggplot(data = train, aes(x = AgeCat, fill = Survived)) + 
    geom_bar(position = &amp;quot;dodge&amp;quot;) + 
    theme_bw()

grid.arrange(p1, p2, p3, layout_matrix = cbind(c(1,3), c(2,3)), widths=c(2,3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/DEDA-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;The percentage of survival is about 38.4%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   train$Sex train$Survived
## 1    female       74.20382
## 2      male       18.89081&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   train$AgeCat train$Survived
## 1        adult       39.02439
## 2        child       58.44156
## 3      missing       29.37853
## 4       senior       22.72727&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In light of the upper part of the following plot, it seems that having family members on board of the Titanic was not very crucial for survival. However, when looking to the second one, it appears clearly that being a member of a small family was relatively more important for survival, mainly because small children were always accompanied by &lt;strong&gt;at least&lt;/strong&gt; one of their parents.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# barplot: survived or not according to has a family on board or not
p1 &amp;lt;- ggplot(data = train, aes(x = HasFam, fill = Survived)) + 
    geom_bar(position = &amp;quot;dodge&amp;quot;) + 
    theme_bw()

# barplot: survived or not according to the category of the family
p2 &amp;lt;- ggplot(data = train, aes(x = FamCat, fill = Survived)) + 
    geom_bar(position = &amp;quot;dodge&amp;quot;) + guides(fill = FALSE) + 
    theme_bw()

grid.arrange(p1, p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   train$HasFam train$Survived
## 1           no       30.35382
## 2          yes       50.56497&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   train$FamCat train$Survived
## 1          big       14.89362
## 2       single       30.35382
## 3        small       56.02606&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wealth also seems to have an import impact on survival. It appears on the two following plots that having a cabin or being a first class passenger increased clearly the chance of survival.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# barplot: survived or not according to having a cabin or not
p2 &amp;lt;- ggplot(data = train, aes(x = HasCab, fill = Survived)) + 
    geom_bar(position = &amp;quot;dodge&amp;quot;) + 
    theme_bw()

# barplot: survival or not according to passenger class
p1 &amp;lt;-  ggplot(data = train, aes(x = Pclass, fill = Survived)) + 
    geom_bar(position = &amp;quot;dodge&amp;quot;) + guides(fill = FALSE) + 
    theme_bw()

grid.arrange(p1, p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   train$Pclass train$Survived
## 1            1       62.96296
## 2            2       47.28261
## 3            3       24.23625&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   train$HasCab train$Survived
## 1           no       29.98544
## 2          yes       66.66667&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the other side, it appears also clearly that the &lt;strong&gt;overwhelming majority&lt;/strong&gt; of women who bought their tickets more than 50 pounds survived. Since it is not the case for men, the reason could be that the women from first class were the first to leave the Titanic on lifeboats when men of the first class was forced to wait for women and children from other classes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Had the Fare any impact? 
ggplot(train, aes(x = PassengerId, y = Fare, color = Survived)) + 
    geom_point() + 
    theme_bw() + 
    facet_grid(. ~ Sex)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The variable Fare seems also to correlate globally to age categories and can thus be of a good benefit when imputing the missing ageCat values. Nevertheless, to overcome the skewed distribution of the variable Fare, an appropriate less sensitive classifier method should be used (Random Forest or Gradient Boosting Method can do the job in such cases)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# How evolves the Fare according to passengers age (depending on age categories)
ggplot(train,aes(x = Age,y = Fare, color = AgeCat)) + geom_point(shape = 1) + 
     geom_smooth(method=&amp;#39;lm&amp;#39;) + scale_y_continuous(limits = c(0, 300)) + 
    theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# How evolves the Fare according to passengers age (overall)
ggplot(train,aes(x = Age,y = Fare)) + geom_point(color = &amp;quot;red&amp;quot;) + 
    geom_smooth(method=&amp;#39;lm&amp;#39;) + scale_y_continuous(limits = c(0, 300)) + 
    theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/unnamed-chunk-14-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Concerning the passengers titles, the proportions of survival for each category correlate to age categories and sex as pointed out above. One can notice also that certain categories such as Doctors, servicemen or clerics did not survive for the majority (did not even tried to leave the Titanic?).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Relationship between survival and name categories
ggplot(data = train, aes(x = NameCat, fill = Survived)) + 
    geom_bar(position = &amp;quot;dodge&amp;quot;) + 
    theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggregate(train$Survived ~ train$NameCat, FUN= function(x) sum(x == &amp;quot;yes&amp;quot;) / length(x) * 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   train$NameCat train$Survived
## 1            Dr       42.85714
## 2        Master       57.50000
## 3           Mil       40.00000
## 4          Miss       70.27027
## 5            Mr       15.66731
## 6           Mrs       79.36508
## 7         Noble       60.00000
## 8           Rev        0.00000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;curiosity-not-directly-related-to-the-topic-of-the-project&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;CURIOSITY (not directly related to the topic of the project)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Relatioship between passengers title and passengers class
# Difference in social classes (exception for &amp;quot;Master&amp;quot;)
p &amp;lt;- ggplot(data = train, aes(Pclass, fill = Pclass)) + 
    geom_bar() + 
    theme_bw()

p + facet_grid(. ~ NameCat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/curiosity-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Relationship between age categories and passengers class 
# (interesting concerning the category &amp;quot;missing&amp;quot; which seems, according to its
# distribution, to be close to &amp;quot;adult&amp;quot;) (suppose that most of the passengers with 
# &amp;quot;missing&amp;quot; ages are in fact &amp;quot;adults&amp;quot;)
p + facet_grid(. ~ AgeCat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/curiosity-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-imputation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Imputation&lt;/h3&gt;
&lt;p&gt;In order to predict the target variable, it is important to impute all missing values of the predictors. There are many methods for the imputation, each with advantages and inconvenients. For this project, two variables show missing values: Embarked and Age (Fare and Age for test data). Since only two values are missing in Embarked, I do a little investigation to deduct them from the ticket number, provided that it should be related to the place were it was issued.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1. train$Embarked

# passengerID with missing &amp;quot;Embarked&amp;quot;
train$PassengerId[is.na(train$Embarked)] &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  62 830&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# both passengerd have same ticket: 113572 =&amp;gt; they embarked together with the same
# ticket
train$Ticket[is.na(train$Embarked)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;113572&amp;quot; &amp;quot;113572&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check other passengers with ticket of the same prefix =&amp;gt; they most probably bought 
# their tickets from the same embarkment place
train$Embarked[grep(&amp;quot;^1135&amp;quot;, train$Ticket)] &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] C    &amp;lt;NA&amp;gt; S    S    S    S    C    S    &amp;lt;NA&amp;gt;
## Levels: C Q S&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# since most of the last cases have value &amp;quot;S&amp;quot; for Embarkment I give the same for the
# missing ones:
train$Embarked[is.na(train$Embarked)] &amp;lt;- &amp;quot;S&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Concerning the missing values of Age, I decided to use the variable AgeCat instead to avoid imputing numerical values. The missing values are imputed using Random Forest algorithm since it appears, as pointed out above, that the age category of the passengers is related directly to many parameters included in our dataset. Of course, the variable Survived is excluded when training the model.&lt;/p&gt;
&lt;p&gt;Although it is possible to merge training and data set for the computation of a unique model, I prefer to do the computation for each set separately. The chosed classifiers are: “Pclass”, “Sex”, “Embarked”, “Fare,”NameCat“,”HasCab“, and”FamCat&amp;quot; while the target variable is “AgeCat” with three different levels: “child”, “adult”, and “senior”.&lt;/p&gt;
&lt;p&gt;The trainControl was set to use the K-fold cross-validation as it represents a robust method to estimate the model’s accuracy. The choice of k = 5 has been empirically shown to avoid high bias and variance when estimating the test error rate &lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train$Age
# The imputation is predicted
# 1. Select the predictors - &amp;quot;Survived&amp;quot;&amp;quot; should not be selected
trainAgetrain &amp;lt;- train[c(1, 3, 5, 10, 12, 13, 14, 15, 17)]

# keep only observations with AgeCat = &amp;quot;missing&amp;quot; in the test subset
trainAgetest &amp;lt;- trainAgetrain[trainAgetrain$AgeCat == &amp;quot;missing&amp;quot;,]

# exclude AgeCat from the test subset
trainAgetest &amp;lt;- trainAgetest[, -7]

# exclude observations with AgeCat = &amp;quot;missing&amp;quot; in the train subset
trainAgetrain &amp;lt;- trainAgetrain[trainAgetrain$AgeCat != &amp;quot;missing&amp;quot;,] 
trainAgetrain &amp;lt;- trainAgetrain[, -1]

# exclude the level &amp;quot;missing&amp;quot;
trainAgetrain$AgeCat &amp;lt;- factor(trainAgetrain$AgeCat)

# Subset the train data set
set.seed(1962)
subsets &amp;lt;- createDataPartition(y=trainAgetrain$AgeCat, p=0.75, list=FALSE)
subTraining &amp;lt;- trainAgetrain[subsets, ] 
subTesting &amp;lt;- trainAgetrain[-subsets, ]

# train the model
x &amp;lt;- subTraining[, -6] # column 6 is the target variable
y &amp;lt;- subTraining[, 6]
control = trainControl(method = &amp;quot;cv&amp;quot;, number = 5, allowParallel = TRUE)
modelRFtrain &amp;lt;- train(x, y, method = &amp;quot;rf&amp;quot;, trControl = control)  # I use random forest &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#modelRFtrain$results
predRFtrain &amp;lt;- predict(modelRFtrain, subTesting)
cfMatRFtrain &amp;lt;- confusionMatrix(subTesting$AgeCat, predRFtrain)
cfMatRFtrain$overall&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##     0.93785311     0.70165492     0.89152431     0.96857126     0.90395480 
## AccuracyPValue  McnemarPValue 
##     0.07426355            NaN&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cfMatRFtrain$table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Reference
## Prediction adult child senior
##     adult    151     2      0
##     child      4    15      0
##     senior     5     0      0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;importanceRFtrain &amp;lt;- varImp(modelRFtrain,scale = FALSE)
plot(importanceRFtrain, top= 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/titanic_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The obtained model shows a very good accuracy on training subset. The most important predictors are resp. NameCat, Fare and FamCat. This result is not surprising as this relationship appeared clearly in the exploratory data analysis.&lt;/p&gt;
&lt;p&gt;The missing values of AgeCat in train data are then predicted using the obtained RF model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predict for the missing AgeCat in train data set
predAgetrain &amp;lt;- predict(modelRFtrain, trainAgetest)

# values are then injected into train
train$AgeCat[train$PassengerId %in% trainAgetest$PassengerId] &amp;lt;- predAgetrain
train$AgeCat &amp;lt;- factor(train$AgeCat)
# ifelse(train$AgeCat[trainAgetest$PassengerId] == predAge, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adapting-the-test-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adapting the test file&lt;/h3&gt;
&lt;p&gt;In order to predict survival, the test data should be modified in the same way as for the train data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sapply(test, function(x) sum(is.na(x))) # how many observations are missing&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## PassengerId      Pclass        Name         Sex         Age       SibSp 
##           0           0           0           0          86           0 
##       Parch      Ticket        Fare       Cabin    Embarked 
##           0           0           1         327           0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We notice here missing values in Age and Fare. Concerning Fare, and since it is only one value missing, one could simply assign to this observation the median Fare value of its corresponding class.&lt;/p&gt;
&lt;p&gt;The missing values of AgeCat are imputed using the exact same method as for the train data.&lt;/p&gt;
&lt;p&gt;Many titles seen in the train file are missing in test file. At the same time, a new one, &lt;strong&gt;Dona&lt;/strong&gt;, is classified as a noble title.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data Engineering

# 1. test$Name
x &amp;lt;- sub(&amp;quot;.*, &amp;quot;, &amp;quot;&amp;quot;, as.character(test$Name))
test$NameCat &amp;lt;- sub(&amp;quot;\\..*&amp;quot;, &amp;quot;&amp;quot;, x)
test$NameCat[test$NameCat %in% c(&amp;quot;Miss&amp;quot;, &amp;quot;Ms&amp;quot;)] &amp;lt;- &amp;quot;Miss&amp;quot;
test$NameCat[test$NameCat == &amp;quot;Col&amp;quot;] &amp;lt;- &amp;quot;Mil&amp;quot;
test$NameCat[test$NameCat == &amp;quot;Dona&amp;quot;] &amp;lt;- &amp;quot;Noble&amp;quot;

# 2. test$Age
test$AgeCat[test$Age &amp;gt; 0 &amp;amp; test$Age &amp;lt;= 14 &amp;amp; !is.na(test$Age)] &amp;lt;- &amp;quot;child&amp;quot;
test$AgeCat[test$Age &amp;gt; 14 &amp;amp; test$Age &amp;lt;= 60 &amp;amp; !is.na(test$Age)] &amp;lt;- &amp;quot;adult&amp;quot;
test$AgeCat[test$Age &amp;gt; 60 &amp;amp; !is.na(test$Age)] &amp;lt;- &amp;quot;senior&amp;quot;
test$AgeCat[is.na(test$Age)] &amp;lt;- &amp;quot;missing&amp;quot;

# 3. test$Cabin
test$HasCab[!is.na(test$Cabin)] &amp;lt;- &amp;quot;yes&amp;quot;
test$HasCab[is.na(test$Cabin)] &amp;lt;- &amp;quot;no&amp;quot;

# 4. Add test$HasFam
test$HasFam[test$SibSp != 0 | test$Parch != 0] &amp;lt;- &amp;quot;yes&amp;quot;
test$HasFam[test$SibSp == 0 &amp;amp; test$Parch == 0] &amp;lt;- &amp;quot;no&amp;quot;

# 5. Add test$Famcat
test$FamCat[test$SibSp + test$Parch &amp;lt;= 4] &amp;lt;- &amp;quot;small&amp;quot;
test$FamCat[test$SibSp + test$Parch &amp;gt; 4] &amp;lt;- &amp;quot;big&amp;quot;
test$FamCat[test$SibSp + test$Parch == 0] &amp;lt;- &amp;quot;single&amp;quot;

# 6. missing Fare: PassengerId = 1044 with Pclass = 3 -&amp;gt; use the median for pclass
test$Fare[test$PassengerId == 1044] &amp;lt;- median(test$Fare[test$Pclass == 3], na.rm = TRUE)

test$NameCat &amp;lt;- factor(test$NameCat)
test$AgeCat &amp;lt;- factor(test$AgeCat)
test$HasCab &amp;lt;- factor(test$HasCab)
test$HasFam &amp;lt;- factor(test$HasFam)
test$FamCat &amp;lt;- factor(test$FamCat)

#Imputation of missing test$Age
testAgetrain &amp;lt;- test[c(1, 2, 4, 9, 11, 12, 13, 14, 16)]
testAgetest &amp;lt;- testAgetrain[testAgetrain$AgeCat == &amp;quot;missing&amp;quot;,]
testAgetest &amp;lt;- testAgetest[, -7]                                    # test file
testAgetrain &amp;lt;- testAgetrain[testAgetrain$AgeCat != &amp;quot;missing&amp;quot;,]
testAgetrain &amp;lt;- testAgetrain[, -1]                                  # train file
testAgetrain$AgeCat &amp;lt;- factor(testAgetrain$AgeCat)                  # in case of different levels
set.seed(1962)
subsets &amp;lt;- createDataPartition(y=testAgetrain$AgeCat, p=0.75, list=FALSE)
subTraining &amp;lt;- testAgetrain[subsets, ] 
subTesting &amp;lt;- testAgetrain[-subsets, ]
x &amp;lt;- subTraining[, -6]                                              # column 6 is the target variable
y &amp;lt;- subTraining[, 6]
control = trainControl(method = &amp;quot;cv&amp;quot;, number = 5, allowParallel = TRUE)
modelRFtest &amp;lt;- train(x, y, method = &amp;quot;rf&amp;quot;, trControl = control)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#modelRFtest$results
predRFtest &amp;lt;- predict(modelRFtest, subTesting)
cfMatRFtest &amp;lt;- confusionMatrix(subTesting$AgeCat, predRFtest)
cfMatRFtest$overall&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##      0.8765432      0.4409938      0.7846553      0.9391798      0.8641975 
## AccuracyPValue  McnemarPValue 
##      0.4510839            NaN&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cfMatRFtest$table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Reference
## Prediction adult child senior
##     adult     66     5      1
##     child      2     5      0
##     senior     2     0      0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predtestAgetest &amp;lt;- predict(modelRFtest, testAgetest)
test$AgeCat[test$PassengerId %in% testAgetest$PassengerId] &amp;lt;- predtestAgetest
test$AgeCat &amp;lt;- factor(test$AgeCat)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction-of-survival&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prediction of survival&lt;/h3&gt;
&lt;p&gt;Now that all the data is processed and missing values imputed, lets compute the final model and predict survival among the passengers in the test list.&lt;/p&gt;
&lt;p&gt;Contrarily to the imputation of AgeCat, the boosting methods proved to be more effective as they better accomodate:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;the small variance of the selected predictors provided that the outcome reduces to only two values: Survived or not,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the high bias that could be induced by the use of categorical predictors.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For these reasons, I decided to use the gradient boosting method (GBM) to predict the survival.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;predictor variables&lt;/strong&gt; are: “Pclass”, “Sex”, “Fare”“,”Embarked“,”NameCat“,”AgeCat“,”HasCab“, and”FamCat“.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;target variable&lt;/strong&gt; is “Survived”.&lt;/p&gt;
&lt;p&gt;The train data consists in 891 observations while the test data consists in 418.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# RF for train and test
trainFin &amp;lt;- train[c(2, 3, 5, 10, 12, 13, 14, 15, 17)]
#trainFin$Survived &amp;lt;- factor(trainFin$Survived)
testFin &amp;lt;- test[c(1, 2, 4, 9, 11, 12, 13, 14, 16)]
set.seed(1962)
subsets &amp;lt;- createDataPartition(y=trainFin$AgeCat, p=0.75, list=FALSE)
subTraining &amp;lt;- trainFin[subsets, ]
subTesting &amp;lt;- trainFin[-subsets, ]
x &amp;lt;- subTraining[, -1] # column 1 is the target variable
y &amp;lt;- subTraining[, 1]
control = trainControl(method = &amp;quot;boot&amp;quot;, number = 5, allowParallel = TRUE)
modelGBMFin &amp;lt;- train(x, y, method = &amp;quot;gbm&amp;quot;, trControl = control)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#modelGBMFin$results
predGBMFin &amp;lt;- predict(modelGBMFin, subTesting)
cfMatGBMFin &amp;lt;- confusionMatrix(subTesting$Survived, predGBMFin)
cfMatGBMFin$overall&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##   8.108108e-01   5.901899e-01   7.530008e-01   8.601154e-01   6.576577e-01 
## AccuracyPValue  McnemarPValue 
##   3.427043e-07   2.800872e-01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cfMatGBMFin$table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Reference
## Prediction  no yes
##        no  121  17
##        yes  25  59&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predGBMFin &amp;lt;- predict(modelGBMFin, testFin)
sol &amp;lt;- ifelse(predGBMFin == &amp;quot;yes&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;0&amp;quot;)
z &amp;lt;- data.frame(test$PassengerId, sol)
colnames(z) &amp;lt;- c(&amp;quot;PassengerId&amp;quot;, &amp;quot;Survived&amp;quot;)
write.csv(z, &amp;quot;Res.csv&amp;quot;, row.names=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model obtained had an accuracy of 0.77511. Nevertheless, another model obtained just by replacing FamCat with HasFam reaches an accuracy of 0.80382 and ended much higher on the leaderboard.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bibliography&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Mersey, Lord (1999) [1912]. The Loss of the Titanic, 1912. The Stationery Office. ISBN 978-0-11-702403-8&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.encyclopedia-titanica.org/titanic-statistics.html&#34; class=&#34;uri&#34;&gt;https://www.encyclopedia-titanica.org/titanic-statistics.html&lt;/a&gt;&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/&#34; class=&#34;uri&#34;&gt;http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/&lt;/a&gt;&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predicting correctness of weight lifting exercises</title>
      <link>https://slamara.github.io/project/weightlifting/</link>
      <pubDate>Wed, 08 Aug 2018 17:17:52 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/weightlifting/</guid>
      <description>&lt;div id=&#34;background&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;Many fitness enthusiasts focuse mainly on how often they practice and how heavy they can lift, and usually neglect to verify whether they do it well or not. Yet, injuries due to unnatural movements and overexertion were proved to be the most commonly occuring ones during free weight activities &lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. With the increasing use of devices that are equipped with accelerometers it is now possible to quantify how well a movement is done.&lt;/p&gt;
&lt;p&gt;In this project, I use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to a scientific study. During this experiment, the participants were asked to perform barbell lifts in 5 different ways: one correct and four incorrect. More information is available from the website &lt;a href=&#34;http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har&#34;&gt;here&lt;/a&gt;. See the section on the Weight Lifting Exercise Dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;The training data for this project is available here: &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&#34;&gt;pml-training.csv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The test data is available here: &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&#34;&gt;pml-testing.csv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For reproducibility, the following libraries should be installed and loaded in order to accomplish all processing steps included in this report:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
library(rattle)
library(parallel)
library(doParallel)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Processing&lt;/h3&gt;
&lt;p&gt;When it comes to machine learning, one should consider the quality of data as most crucial element. Incomplete, irrelevant and inaccurate data sets are all sources of errors that will be inevitably incorporated in any machine learning analysis, giving to the observation &lt;em&gt;“garbage in, garbage out”&lt;/em&gt; all its sense. For such high-quality demanding analysis, one should thus spend enough time wrangling data efficiently.&lt;br /&gt;
Fortunately, the tidying of the data was already done &lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; (a data summary is given &lt;a href=&#34;https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-predictionSummary.md&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Nonetheless, a look at the data content shows at first sight a substantive occurrence of various missing and/or irrelevant inputs (“NA”, “”, #Div/0!“) which need to be normalized from the notational point of view.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#download.file(&amp;quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&amp;quot;, &amp;quot;pml-training.csv&amp;quot;)
#download.file(&amp;quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&amp;quot;, &amp;quot;pml-testing.csv&amp;quot;)
preTest &amp;lt;- read.csv(&amp;quot;./weightLifting/pml-testing.csv&amp;quot;, na.strings = c(&amp;quot;NA&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;#DIV/0!&amp;quot;))
preTrain &amp;lt;- read.csv(&amp;quot;./weightLifting/pml-training.csv&amp;quot;, na.strings = c(&amp;quot;NA&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;#DIV/0!&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is then processed as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;the 1st and 6th columns (resp. observation’s number and “new_number”) are irrelevant in this context and are excluded,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the columns with down to 90% of “NA” values are also excluded,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In case, two predictors present high pairwise correlation (in term of the Pearson’s correlation coefficient &lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;), only the best is kept (the one with the smallest mean absolute correlation). The cut-off value of the Pearson’s correlation coefficient is usually chosen to be higher than 0.75. Since we use variables recorded by body sensors that could be mutually influenced, I choose a bit higher value of 0.8,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the predictors with near-zero variance should be excluded. However, all of them were already excluded within the previous steps,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;we subset the training data into training and testing sets and set up the training run with the (x, y) syntax.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1. Exclude the 1st and 6th columns
preTest &amp;lt;- preTest[, -c(1, 6)] 
preTrain &amp;lt;- preTrain[, -c(1, 6)]

# 2. Exclude the columns with too many NAs (cut off at 90%)
training &amp;lt;- preTrain[,colSums(is.na(preTrain))/nrow(preTrain) &amp;lt; 0.9]
cIndex &amp;lt;- which(!(colnames(preTrain) %in% colnames(training)))
testing &amp;lt;- preTest[, - cIndex]

# 3. Exclude the predictors that are highly correlated
analyseDataTrain &amp;lt;- sapply(training[,-58], as.numeric) # column 58 is the target variable
corMat &amp;lt;- cor(analyseDataTrain)
highCor &amp;lt;- findCorrelation(corMat, cutoff=0.8)
trainingCor &amp;lt;- training[, -highCor]
testingCor &amp;lt;- testing[, -highCor]

# 4. Exclude the predictors with very small variance
#nsv &amp;lt;- nearZeroVar(trainingCor) # in this case nsv is empty

# 5. Subset the data
set.seed(123)
subsets &amp;lt;- createDataPartition(y=trainingCor$classe, p=0.75, list=FALSE)
subTrainingCor &amp;lt;- trainingCor[subsets, ] 
subTestingCor &amp;lt;- trainingCor[-subsets, ]
x &amp;lt;- subTrainingCor[, -44] # column 44 is the target variable
y &amp;lt;- subTrainingCor[, 44]
rm(preTest, preTrain,testing, training, cIndex, analyseDataTrain, corMat, highCor, subsets)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;computer-configuration-used-in-this-project&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Computer configuration used in this project&lt;/h3&gt;
&lt;table style=&#34;width:100%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;24%&#34; /&gt;
&lt;col width=&#34;75%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Computer&lt;/th&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Samsung Series 5 Ultra&lt;/td&gt;
&lt;td&gt;* Operating system: Windows 10 (64 Bits)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;* Processor: Intel Corei5 3337U @ 1.80GHz up to 2.7GHz (2 cores, 4 threads)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;* RAM: 8 Gb&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;* Disk: 512 Gb SSD&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-up-the-parallel-environment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting up the parallel environment&lt;/h3&gt;
&lt;p&gt;To improve processing time I decided to use caret on parallel environment. This accomplished as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cluster &amp;lt;- makeCluster(detectCores() - 1)  # 1 core is left for OS
registerDoParallel(cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model fitting&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;trainControl&lt;/strong&gt; was set to use the K-fold cross-validation as it represents a robust method to estimate the model’s accuracy. The choice of k = 5 has been empirically shown to avoid high bias and variance when estimating the test error rate &lt;a href=&#34;#fn4&#34; class=&#34;footnoteRef&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this experiment, 6 participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;exactly according to the specification (Class A)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;throwing the elbows to the front (Class B)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;lifting the dumbbell only halfway (Class C)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;lowering the dumbbell only halfway (Class D)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;throwing the hips to the front (Class E).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the class A corresponds to the specified execution of the exercise, the other 4 classes correspond to common mistakes.&lt;/p&gt;
&lt;p&gt;As we try to predict these behaviors through predictor variables, the &lt;strong&gt;target variable&lt;/strong&gt;, “classe”, is thus a factor with 5 different levels corresponding to the five preceding cases.&lt;/p&gt;
&lt;p&gt;To illustrate the difference in accuracy according to the method used, three different machine learning methods are chosen:&lt;/p&gt;
&lt;div id=&#34;decision-tree&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1. Decision tree&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
control = trainControl(method = &amp;quot;cv&amp;quot;, number = 5, allowParallel = TRUE)
system.time(modelRPART &amp;lt;- train(x, y, method = &amp;quot;rpart&amp;quot;, trControl = control)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        User      System verstrichen 
##        6.00        0.12       20.12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modelRPART$results&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           cp  Accuracy     Kappa  AccuracySD    KappaSD
## 1 0.03398842 0.5565960 0.4378718 0.009052306 0.01130354
## 2 0.03792842 0.5119644 0.3733120 0.036654830 0.05876857
## 3 0.06728852 0.3606495 0.1255237 0.104492903 0.17188225&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predRPART &amp;lt;- predict(modelRPART, subTestingCor)
cfMatRPART &amp;lt;- confusionMatrix(subTestingCor$classe, predRPART)
cfMatRPART$overall&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##   5.309951e-01   4.053248e-01   5.169111e-01   5.450422e-01   4.783850e-01 
## AccuracyPValue  McnemarPValue 
##   9.423710e-14   0.000000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cfMatRPART$table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Reference
## Prediction    A    B    C    D    E
##          A 1180   31  184    0    0
##          B  158  317  472    0    2
##          C   16   32  806    0    1
##          D   71  152  521    0   60
##          E   69  168  363    0  301&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fancyRpartPlot(modelRPART$finalModel)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/weightLifting_files/figure-html/ModelPredRPART-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model obtained has a low accuracy of 55,66% and cannot be used to predict the results of the test data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-boosting-method&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2. Gradient boosting method&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123) 
elapsed &amp;lt;- system.time(modelGBM &amp;lt;- train(x, y, method = &amp;quot;gbm&amp;quot;, trControl = control)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        User      System verstrichen 
##       33.14        0.16      172.27&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modelGBM$results&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   shrinkage interaction.depth n.minobsinnode n.trees  Accuracy     Kappa
## 1       0.1                 1             10      50 0.7842776 0.7262820
## 4       0.1                 2             10      50 0.9287265 0.9097723
## 7       0.1                 3             10      50 0.9660959 0.9570896
## 2       0.1                 1             10     100 0.8660808 0.8303878
## 5       0.1                 2             10     100 0.9807720 0.9756724
## 8       0.1                 3             10     100 0.9938168 0.9921793
## 3       0.1                 1             10     150 0.9033830 0.8776612
## 6       0.1                 2             10     150 0.9924579 0.9904598
## 9       0.1                 3             10     150 0.9974179 0.9967340
##    AccuracySD     KappaSD
## 1 0.007445123 0.009479860
## 4 0.005811340 0.007363580
## 7 0.002180500 0.002753041
## 2 0.008072280 0.010331460
## 5 0.001602403 0.002032181
## 8 0.001883989 0.002383054
## 3 0.008613351 0.010989106
## 6 0.002235010 0.002827177
## 9 0.001658198 0.002097462&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predGBM &amp;lt;- predict(modelGBM, subTestingCor) 
cfMatGBM &amp;lt;- confusionMatrix(subTestingCor$classe, predGBM)
cfMatGBM$overall&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##      0.9977569      0.9971627      0.9959901      0.9988798      0.2846656 
## AccuracyPValue  McnemarPValue 
##      0.0000000            NaN&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cfMatGBM$table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Reference
## Prediction    A    B    C    D    E
##          A 1395    0    0    0    0
##          B    1  948    0    0    0
##          C    0    1  853    1    0
##          D    0    0    5  798    1
##          E    0    0    0    2  899&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model from the GBM method presents very high accuracy on train set (99.74%) as well as on test set (99,78%). Since these values are too close, I assume that there is no overfitting on the train set. On the other hand, one can notice that the test accuracy is slightly better than the train one. Yet, the difference is in fact too small as it corresponds to &lt;strong&gt;less than&lt;/strong&gt; two additional good predictions on the test set and could be induced by the smallest number of observations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3. Random forest&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
system.time(modelRF &amp;lt;- train(x, y, method = &amp;quot;rf&amp;quot;, trControl = control))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        User      System verstrichen 
##       58.67        1.32      446.53&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modelRF$results &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mtry  Accuracy     Kappa  AccuracySD     KappaSD
## 1    2 0.9952433 0.9939829 0.002150399 0.002720445
## 2   22 0.9986407 0.9982806 0.001749896 0.002213406
## 3   43 0.9967383 0.9958746 0.001743711 0.002205343&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predRF &amp;lt;- predict(modelRF, subTestingCor)
cfMatRF &amp;lt;- confusionMatrix(subTestingCor$classe, predRF)
rm(control)
cfMatRF$overall&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##      0.9989804      0.9987104      0.9976223      0.9996689      0.2844617 
## AccuracyPValue  McnemarPValue 
##      0.0000000            NaN&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cfMatRF$table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Reference
## Prediction    A    B    C    D    E
##          A 1395    0    0    0    0
##          B    0  948    1    0    0
##          C    0    1  854    0    0
##          D    0    0    3  801    0
##          E    0    0    0    0  901&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The obtained model shows very high accuracies. As for the GBM method, the train and test accuracies are once again too close with the latter slightly better than the first. There is thus no overfitting on the train set.&lt;/p&gt;
&lt;p&gt;To avoid doubts on the resampling method, I computed a new model using &lt;strong&gt;&lt;em&gt;repeatedcv&lt;/em&gt;&lt;/strong&gt; on the same data set. The accuracies obtained with 5 repeats (99,88%) are exactly the same and are very close to the ones obtained previously.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-between-the-different-methods&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparison between the different methods&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- resamples(list(RPART=modelRPART, GBM=modelGBM, RF=modelRF))
results$timings &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Everything FinalModel Prediction
## RPART      20.12       0.39         NA
## GBM       172.27      32.25         NA
## RF        446.53      59.48         NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bwplot(results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/weightLifting_files/figure-html/compModels-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Apart of the RPART method, the models obtained with the GBM and RF methods fit the test data very well. The standard deviations of the accuracies are also very small which suggests that the data collected with the body sensors is very accurate. The big difference in processing time between the GBM and RF methods (about the double) poses the problem of performance when using a time-consuming method for just a little improvement. Of course, for this project, one can afford using the RF method to predict the cases from the test data. However, once the data becomes very big, it is important to assess all these different aspects taking into consideration the acceptable margin of error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predictors-importance-in-random-forest&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predictor’s importance in Random Forest&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;importanceRF &amp;lt;- varImp(modelRF,scale = FALSE)
plot(importanceRF, top= 10) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/weightLifting_files/figure-html/predImportance-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is very interesting to notice that the most important predictors used in RF method are the &lt;strong&gt;&lt;em&gt;raw_timestamp_part_1&lt;/em&gt;&lt;/strong&gt; and the &lt;strong&gt;&lt;em&gt;num_window&lt;/em&gt;&lt;/strong&gt;. Since the data was recorded by body sensors using a sliding time window with different durations, it is therefore necessary to look at the observations according to their corresponding time stamp and window number. While this behavior is intuitive for a human being, the fact remains that it is not obvious for a machine. Fortunately, the RF algorithm permitted to learn from the data enough to spot this fact.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-to-the-test-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Application to the test data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(modelRF, testingCor) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 100% accuracy
#rm(list = ls())
stopCluster(cluster)
registerDoSEQ()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bibliography&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Gray, S. E., &amp;amp; Finch, C. F. (2015). The causes of injuries sustained at fitness facilities presenting to Victorian emergency departments-identifying the main culprits. Injury epidemiology, 2(1), 6.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Velloso, E., Bulling, A., Gellersen, H., Ugulino, W., &amp;amp; Fuks, H. (2013, March). Qualitative activity recognition of weight lifting exercises. In Proceedings of the 4th Augmented Human International Conference (pp. 116-123). ACM.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Pearson, K. (1896). Mathematical contributions to the theory of evolution. III. Regression, heredity, and panmixia. Philosophical Transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character, 187, 253-318.&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/&#34; class=&#34;uri&#34;&gt;http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/&lt;/a&gt;&lt;a href=&#34;#fnref4&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Impact of severe weather events in the United States</title>
      <link>https://slamara.github.io/project/severeweather/</link>
      <pubDate>Wed, 08 Aug 2018 17:08:23 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/severeweather/</guid>
      <description>&lt;div id=&#34;synopsis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Synopsis&lt;/h2&gt;
&lt;p&gt;Severe weather events have shown in the last decades an increasing threat on population safety and a costly impact on economy. Hence, their potential consequences should be studied for a better mitigation of their inherent risk. The U.S. National Oceanic and Atmospheric Administration (NOAA) has implemented a catalog where all characteristics of major storms and weather events in the United States from 1950 to November 2011 were collected. These characteristics include when and where these events occurred, as well as estimates of any fatalities, injuries, property and crop damages. The aim of the present analysis is to identify events that are most harmful to population health and economy across the United States.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data processing&lt;/h2&gt;
&lt;p&gt;Before doing such analysis, the homogeneity and completeness of the catalog should be assessed to insure reliability of the results. Unfortunately, I am forced in this study to use the catalog as it is since no further details are given.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(utils)
library(ggplot2)
library(gridExtra)
library(scales)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#download.file(&amp;quot;https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2&amp;quot;, &amp;quot;StormData.csv.bz2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StormData &amp;lt;- read.csv(&amp;quot;./severeWeather/StormData.csv.bz2&amp;quot;)
dim(StormData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 902297     37&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(StormData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;STATE__&amp;quot;    &amp;quot;BGN_DATE&amp;quot;   &amp;quot;BGN_TIME&amp;quot;   &amp;quot;TIME_ZONE&amp;quot;  &amp;quot;COUNTY&amp;quot;    
##  [6] &amp;quot;COUNTYNAME&amp;quot; &amp;quot;STATE&amp;quot;      &amp;quot;EVTYPE&amp;quot;     &amp;quot;BGN_RANGE&amp;quot;  &amp;quot;BGN_AZI&amp;quot;   
## [11] &amp;quot;BGN_LOCATI&amp;quot; &amp;quot;END_DATE&amp;quot;   &amp;quot;END_TIME&amp;quot;   &amp;quot;COUNTY_END&amp;quot; &amp;quot;COUNTYENDN&amp;quot;
## [16] &amp;quot;END_RANGE&amp;quot;  &amp;quot;END_AZI&amp;quot;    &amp;quot;END_LOCATI&amp;quot; &amp;quot;LENGTH&amp;quot;     &amp;quot;WIDTH&amp;quot;     
## [21] &amp;quot;F&amp;quot;          &amp;quot;MAG&amp;quot;        &amp;quot;FATALITIES&amp;quot; &amp;quot;INJURIES&amp;quot;   &amp;quot;PROPDMG&amp;quot;   
## [26] &amp;quot;PROPDMGEXP&amp;quot; &amp;quot;CROPDMG&amp;quot;    &amp;quot;CROPDMGEXP&amp;quot; &amp;quot;WFO&amp;quot;        &amp;quot;STATEOFFIC&amp;quot;
## [31] &amp;quot;ZONENAMES&amp;quot;  &amp;quot;LATITUDE&amp;quot;   &amp;quot;LONGITUDE&amp;quot;  &amp;quot;LATITUDE_E&amp;quot; &amp;quot;LONGITUDE_&amp;quot;
## [36] &amp;quot;REMARKS&amp;quot;    &amp;quot;REFNUM&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;correct-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correct the data&lt;/h3&gt;
&lt;p&gt;As many observations in variable EVTYPE (representing the different weather events) can be gathered in one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(StormData$EVTYPE[grep(&amp;quot;fire&amp;quot;, StormData$EVTYPE, ignore.case = TRUE)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] WILD FIRES        WILDFIRE          WILD/FOREST FIRE 
##  [4] GRASS FIRES       LIGHTNING FIRE    FOREST FIRES     
##  [7] WILDFIRES         WILD/FOREST FIRES BRUSH FIRES      
## [10] BRUSH FIRE        RED FLAG FIRE WX 
## 985 Levels:    HIGH SURF ADVISORY  COASTAL FLOOD ... WND&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The column should be updated for the events: wind, heat, fire, flood and hurricane:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elements &amp;lt;- c(&amp;quot;wind&amp;quot;, &amp;quot;heat&amp;quot;, &amp;quot;fire&amp;quot;, &amp;quot;flood&amp;quot;, &amp;quot;hurricane&amp;quot;)

StormDataCorr &amp;lt;- StormData

levels(StormDataCorr$EVTYPE) &amp;lt;- c(levels(StormDataCorr$EVTYPE), &amp;quot;FIRE&amp;quot;)
for (i in elements){
    StormDataCorr$EVTYPE[grep(i, StormDataCorr$EVTYPE, ignore.case = TRUE)] &amp;lt;- toupper(i)
}

unique(StormDataCorr$EVTYPE[grep(&amp;quot;fire&amp;quot;, StormDataCorr$EVTYPE, ignore.case = TRUE)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FIRE
## 986 Levels:    HIGH SURF ADVISORY  COASTAL FLOOD ... FIRE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-the-impact-on-population-health-fatalities-and-injuries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compute the impact on population health (fatalities and injuries)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AggreFatInj &amp;lt;- function(data = StormDataCorr, field) {
    aggre &amp;lt;- aggregate(data[,field], by = list(EVTYPE = data$EVTYPE), FUN = sum)
    aggreOrd &amp;lt;- aggre[order(-aggre$x),]
    aggreOrd &amp;lt;- aggreOrd[which(aggreOrd$x&amp;gt;0),]
    aggreOrd
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-the-economical-impact-property-and-crop-damages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compute the economical impact (property and crop damages)&lt;/h3&gt;
&lt;p&gt;Before computing the correct amount of property and crop damages, we should take in account the appropriate exponentials. First, I set up a dictionary:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exponent &amp;lt;- cbind(exp = c(&amp;quot;H&amp;quot;,&amp;quot;h&amp;quot;,&amp;quot;K&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;M&amp;quot;,&amp;quot;m&amp;quot;,&amp;quot;B&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;0&amp;quot;,&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;,&amp;quot;6&amp;quot;,&amp;quot;7&amp;quot;,&amp;quot;8&amp;quot;), 
                  value = c(10^2,10^2,10^3,10^3,10^6,10^6,10^9,10^9,1,10,10,10,10,10,10,10,10,10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then I compute the correct impact with the following function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggrePropCrop &amp;lt;- function(data = StormDataCorr, field, fieldExp) {
    
    DataDMG &amp;lt;- cbind(EVTYPE = as.character(data$EVTYPE), field = as.numeric(data[,field]) * as.numeric(exponent[match(data[,fieldExp],exponent),2]))
    
    aggreDMG &amp;lt;- aggregate(as.numeric(DataDMG[,2]), 
                          by = list(EVTYPE = DataDMG[,1]), 
                          FUN = sum, na.rm = TRUE)
    
    aggreOrd &amp;lt;- aggreDMG[order(-aggreDMG$x),]
    
    aggreOrd &amp;lt;- aggreOrd[which(aggreOrd$x&amp;gt;0),]
    
    aggreOrd
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;div id=&#34;fatalities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fatalities:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggreFatOrd &amp;lt;- AggreFatInj(StormDataCorr, &amp;quot;FATALITIES&amp;quot;)
head(aggreFatOrd, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           EVTYPE    x
## 527      TORNADO 5633
## 177         HEAT 3138
## 109        FLOOD 1525
## 607         WIND 1451
## 286    LIGHTNING  816
## 379  RIP CURRENT  368
## 14     AVALANCHE  224
## 609 WINTER STORM  206
## 380 RIP CURRENTS  204
## 102 EXTREME COLD  160&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;injuries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Injuries:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggreInjOrd &amp;lt;- AggreFatInj(StormDataCorr, &amp;quot;INJURIES&amp;quot;)
head(aggreInjOrd, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           EVTYPE     x
## 527      TORNADO 91346
## 607         WIND 11498
## 177         HEAT  9224
## 109        FLOOD  8604
## 286    LIGHTNING  5230
## 254    ICE STORM  1975
## 620         FIRE  1608
## 149         HAIL  1361
## 240    HURRICANE  1328
## 609 WINTER STORM  1321&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FatPlot &amp;lt;- ggplot(head(aggreFatOrd,10), aes(x=reorder(EVTYPE, -x), y = x)) +
    geom_bar(stat=&amp;quot;identity&amp;quot;, width = 0.5) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    theme(plot.title = element_text(hjust = 0.5)) + 
    labs(title=&amp;quot;Total fatalities by severe weather\n events in the US&amp;quot;, 
         x=&amp;quot;Severe weather elements&amp;quot;, 
         y = &amp;quot;Number of fatalities&amp;quot;)

InjPlot &amp;lt;- ggplot(head(aggreInjOrd,10), aes(x=reorder(EVTYPE, -x), y = x)) +
    geom_bar(stat=&amp;quot;identity&amp;quot;, width = 0.5) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    theme(plot.title = element_text(hjust = 0.5)) + 
    labs(title=&amp;quot;Total Injuries by severe weather\n events in the US&amp;quot;, 
         x=&amp;quot;Severe weather elements&amp;quot;, 
         y = &amp;quot;Number of Injuries&amp;quot;)

grid.arrange(FatPlot, InjPlot, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/severeWeather_files/figure-html/plot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;st-fact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1st fact:&lt;/h3&gt;
&lt;p&gt;From the tables and plots it appears clearly that &lt;strong&gt;TORNADOES&lt;/strong&gt; are the &lt;strong&gt;most harmful with respect to the population health&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;property-damage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Property damage:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggrePropOrd &amp;lt;- aggrePropCrop(StormDataCorr, &amp;quot;PROPDMG&amp;quot;, &amp;quot;PROPDMGEXP&amp;quot;)
head(aggrePropOrd, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               EVTYPE            x
## 110            FLOOD 167523218973
## 241        HURRICANE  84656180010
## 528          TORNADO  56937162837
## 448      STORM SURGE  43323536000
## 608             WIND  17742639462
## 150             HAIL  15732269877
## 106             FIRE   8501628500
## 541   TROPICAL STORM   7703890550
## 610     WINTER STORM   6688497260
## 449 STORM SURGE/TIDE   4641188000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;crop-damage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Crop damage:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggreCropOrd &amp;lt;- aggrePropCrop(StormDataCorr, &amp;quot;CROPDMG&amp;quot;, &amp;quot;CROPDMGEXP&amp;quot;)
head(aggreCropOrd, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           EVTYPE           x
## 64       DROUGHT 13972566000
## 110        FLOOD 12267259100
## 241    HURRICANE  5505292800
## 255    ICE STORM  5022113500
## 150         HAIL  3025954650
## 608         WIND  2159305250
## 102 EXTREME COLD  1292973000
## 134 FROST/FREEZE  1094086000
## 178         HEAT   904469280
## 186   HEAVY RAIN   733399800&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PropPlot &amp;lt;- ggplot(head(aggrePropOrd,10), aes(x=reorder(EVTYPE, -x), y = x)) + 
    geom_bar(stat=&amp;quot;identity&amp;quot;, width = 0.5) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size=12)) +
    theme(plot.title = element_text(hjust = 0.5)) +
    scale_y_continuous(labels = comma) + 
    labs(title=&amp;quot;Total amount of property damages\n by severe weather events in the US&amp;quot;, 
         x=&amp;quot;Severe weather elements&amp;quot;, 
         y = &amp;quot;Amount of property damages&amp;quot;)

CropPlot &amp;lt;- ggplot(head(aggreCropOrd,10), aes(x=reorder(EVTYPE, -x), y = x)) + 
    geom_bar(stat=&amp;quot;identity&amp;quot;, width = 0.5) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size=12)) +
    theme(plot.title = element_text(hjust = 0.5)) + 
    scale_y_continuous(labels = comma) + 
    labs(title=&amp;quot;Total amount of crop damages\n by severe weather events in the US&amp;quot;, 
         x=&amp;quot;Severe weather elements&amp;quot;, 
         y = &amp;quot;Amount of crop damages&amp;quot;)

grid.arrange(PropPlot, CropPlot, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/severeWeather_files/figure-html/plot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nd-fact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2nd fact&lt;/h3&gt;
&lt;p&gt;From the tables and plots, it appears clearly that &lt;strong&gt;FLOODS&lt;/strong&gt; and &lt;strong&gt;DROUGHTS&lt;/strong&gt; have the &lt;strong&gt;greatest economic consequences&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Influence of automatic and manual transmission on fuel consumption</title>
      <link>https://slamara.github.io/project/fuelconsumption/</link>
      <pubDate>Wed, 08 Aug 2018 16:53:36 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/fuelconsumption/</guid>
      <description>&lt;div id=&#34;executive-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Executive summary:&lt;/h2&gt;
&lt;p&gt;The purpose of this project is to assess qualitatively and quantitatively the leverage of automatic and manual transmission (&lt;strong&gt;am&lt;/strong&gt;) on the fuel consumption (&lt;strong&gt;mpg&lt;/strong&gt;) of a selection of 32 cars. This should be done with regards to the intrinsic relationship of &lt;strong&gt;mpg&lt;/strong&gt; and &lt;strong&gt;am&lt;/strong&gt; with 10 other car aspects and performances. The data used in this assignment was published in the 1974 Motors Trend magazine.&lt;/p&gt;
&lt;p&gt;In this short report, I start with an exploratory analysis and rapid graphical representation focusing roughly on the &lt;strong&gt;am&lt;/strong&gt; and &lt;strong&gt;mpg&lt;/strong&gt;. In the second part I examine different models and select the best one inferring the relationship between these variables. The report ends with a conclusion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset “mtcars” can be loaded with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data set consists of 11 different characteristics of 32 car models from the 70’s.&lt;/p&gt;
&lt;p&gt;The box plot computed for &lt;strong&gt;mpg&lt;/strong&gt; with regards to &lt;strong&gt;am&lt;/strong&gt; shows clearly that the manual transmission have a higher &lt;strong&gt;mpg&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(as.factor(am), mpg, data = mtcars, geom = &amp;quot;boxplot&amp;quot;, color = as.factor(am), 
      xlab = &amp;quot;Type of transmission(0: automatic, 1: manual)&amp;quot;, 
      ylab = &amp;quot;Number of miles per gallon&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/fuelConsumption_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, the correlation between &lt;strong&gt;mpg&lt;/strong&gt; and the other parameters shows a stronger relationship between &lt;strong&gt;mpg&lt;/strong&gt; and &lt;strong&gt;wt&lt;/strong&gt;, &lt;strong&gt;cyl&lt;/strong&gt;, &lt;strong&gt;disp&lt;/strong&gt;, &lt;strong&gt;hp&lt;/strong&gt;, &lt;strong&gt;drat&lt;/strong&gt;, &lt;strong&gt;vs&lt;/strong&gt; as compared to &lt;strong&gt;am&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corr &amp;lt;- cor(mtcars$mpg, mtcars)
corr[1, order(-abs(corr[1,]))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        mpg         wt        cyl       disp         hp       drat 
##  1.0000000 -0.8676594 -0.8521620 -0.8475514 -0.7761684  0.6811719 
##         vs         am       carb       gear       qsec 
##  0.6640389  0.5998324 -0.5509251  0.4802848  0.4186840&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hence, I consider that since &lt;strong&gt;mpg&lt;/strong&gt; is strongly correlated with other parameters, it could be misleading to ignore their effects on its relationship with the automatic and manual transmissions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model selection&lt;/h2&gt;
&lt;p&gt;As first look, I compute a regression model with &lt;strong&gt;mpg&lt;/strong&gt; as the outcome and &lt;strong&gt;am&lt;/strong&gt; as the regressor:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- lm(mpg ~ am, mtcars)
summary(fit1)$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Estimate Std. Error   t value     Pr(&amp;gt;|t|)
## (Intercept) 17.147368   1.124603 15.247492 1.133983e-15
## am           7.244939   1.764422  4.106127 2.850207e-04&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimate of the intercept represents the hypothetical fuel efficiency in case of automatic transmission (am = 0) while the estimate of &lt;strong&gt;am&lt;/strong&gt; represents the slope for the case of manual transmission (am = 1).&lt;/p&gt;
&lt;p&gt;Fitting all parameters of mtcars:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- lm(mpg ~ ., mtcars)
summary(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ ., data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4506 -1.6044 -0.1196  1.2193  4.6271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept) 12.30337   18.71788   0.657   0.5181  
## cyl         -0.11144    1.04502  -0.107   0.9161  
## disp         0.01334    0.01786   0.747   0.4635  
## hp          -0.02148    0.02177  -0.987   0.3350  
## drat         0.78711    1.63537   0.481   0.6353  
## wt          -3.71530    1.89441  -1.961   0.0633 .
## qsec         0.82104    0.73084   1.123   0.2739  
## vs           0.31776    2.10451   0.151   0.8814  
## am           2.52023    2.05665   1.225   0.2340  
## gear         0.65541    1.49326   0.439   0.6652  
## carb        -0.19942    0.82875  -0.241   0.8122  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.65 on 21 degrees of freedom
## Multiple R-squared:  0.869,  Adjusted R-squared:  0.8066 
## F-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The higher value of R-squared suggests a better fit for this model. However, the p-values, which represent the significance of each parameter in presence of the others, are very high. Thus, to determine statistically the best fitting model I use the &lt;strong&gt;step&lt;/strong&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best &amp;lt;- step(fit2, direction = &amp;quot;both&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(best)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ wt + qsec + am, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4811 -1.5555 -0.7257  1.4110  4.6610 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   9.6178     6.9596   1.382 0.177915    
## wt           -3.9165     0.7112  -5.507 6.95e-06 ***
## qsec          1.2259     0.2887   4.247 0.000216 ***
## am            2.9358     1.4109   2.081 0.046716 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.459 on 28 degrees of freedom
## Multiple R-squared:  0.8497, Adjusted R-squared:  0.8336 
## F-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to the step function, the best model accounts for &lt;strong&gt;am&lt;/strong&gt;, &lt;strong&gt;wt&lt;/strong&gt; and &lt;strong&gt;qsec&lt;/strong&gt;. The R-squared is in this case significant and the p-values small. The coefficient of &lt;strong&gt;am&lt;/strong&gt; shows an &lt;strong&gt;mpg&lt;/strong&gt; higher of about 2.94 miles per gallon in the case of manual transmission.&lt;/p&gt;
&lt;p&gt;The plots of the residuals Vs. fitted values, the square root of the standard residuals Vs. fitted values, the standard residuals Vs. Leverage and the QQ-plot are given by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(2, 2))
plot(best)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/fuelConsumption_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While the QQ-plot seems to be relatively acceptable, the other plots show that the assumptions of normality and linearity are close to be breached.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The final results don’t permit to be confident in concluding a better efficiency of the manual transmission. All I can say in this case is that the quantification of the difference in fuel efficiency between the automatic and manual transmission is only about 3 miles per gallon with a p confidence of 0.046. Due to the small number of observations, only the inclusion of more data can probably improve the confidence in the qualitative and quantitative findings.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Personal activity monitoring</title>
      <link>https://slamara.github.io/project/activitymonitoring/</link>
      <pubDate>Wed, 08 Aug 2018 16:40:59 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/activitymonitoring/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This project makes use of data from a personal activity monitoring device which counts the number of steps taken by an anonymous individual in 5 minutes intervals throughout the day. The &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip&#34;&gt;dataset&lt;/a&gt; was collected within a period of two months (October and November 2012).&lt;/p&gt;
&lt;p&gt;The variables included in this dataset are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;steps&lt;/strong&gt;: Number of steps taking in a 5-minute interval (missing values are coded as NA)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;date&lt;/strong&gt;: The date on which the measurement was taken in YYYY-MM-DD format&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;interval&lt;/strong&gt;: Identifier for the 5-minute interval in which measurement was taken&lt;/p&gt;
&lt;p&gt;The dataset is stored in a comma-separated-value (CSV) file with a total of 17,568 observations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-and-preprocessing-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading and preprocessing the data:&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(xtable)
library(chron)
library(lattice)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;activity &amp;lt;- read.csv(&amp;quot;./activityMonitoring/activity.csv&amp;quot;)

activity$date &amp;lt;- as.Date(activity$date, format=&amp;quot;%Y-%m-%d&amp;quot;)

str(activity)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    17568 obs. of  3 variables:
##  $ steps   : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ date    : Date, format: &amp;quot;2012-10-01&amp;quot; &amp;quot;2012-10-01&amp;quot; ...
##  $ interval: int  0 5 10 15 20 25 30 35 40 45 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-the-mean-total-number-of-steps-taken-per-day&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is the mean total number of steps taken per day?&lt;/h2&gt;
&lt;p&gt;I first calculate the total number of steps taken per day:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StepsPerDay &amp;lt;- activity %&amp;gt;% na.omit() %&amp;gt;% group_by(date) %&amp;gt;% summarise(TotSteps = sum(steps))

StepsPerDay&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 53 x 2
##    date       TotSteps
##    &amp;lt;date&amp;gt;        &amp;lt;int&amp;gt;
##  1 2012-10-02      126
##  2 2012-10-03    11352
##  3 2012-10-04    12116
##  4 2012-10-05    13294
##  5 2012-10-06    15420
##  6 2012-10-07    11015
##  7 2012-10-09    12811
##  8 2012-10-10     9900
##  9 2012-10-11    10304
## 10 2012-10-12    17382
## # ... with 43 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then, I make a histogram of the total number of steps taken each day:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(StepsPerDay$date, StepsPerDay$TotSteps, type=&amp;quot;h&amp;quot;, lwd=5, col=&amp;quot;red&amp;quot;, 
     xlab=&amp;quot;Days&amp;quot;, 
     ylab=&amp;quot;Number of steps&amp;quot;, 
     main=&amp;quot;Total number of steps taken each day&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/activityMonitoring_files/figure-html/histo1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, I calculate and report the mean and median of the total number of steps taken per day&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(StepsPerDay$TotSteps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10766.19&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median(StepsPerDay$TotSteps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10765&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-the-average-daily-activity-pattern&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is the average daily activity pattern?&lt;/h2&gt;
&lt;p&gt;To show the pattern, I make a time series plot (i.e. type = “l”) of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all days (y-axis)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MeanInterval &amp;lt;- aggregate(activity$steps, 
                          by=list(interval=activity$interval), 
                          FUN = mean, na.rm = TRUE)

plot(MeanInterval$interval, MeanInterval$x, type = &amp;quot;l&amp;quot;, col = &amp;quot;blue&amp;quot;, 
     xlab = &amp;quot;5-minute intervals&amp;quot;, 
     ylab = &amp;quot;Average nummber of steps&amp;quot;, 
     main = &amp;quot;Average number of steps averaged across all days&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/activityMonitoring_files/figure-html/timeSerie1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which 5-minute interval, on average across all the days in the dataset, contains the maximum number of steps?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MeanInterval[which.max(MeanInterval$x),]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     interval        x
## 104      835 206.1698&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;imputing-missing-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Imputing missing values&lt;/h2&gt;
&lt;p&gt;I first calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(is.na(activity$steps))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2304&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a strategy for filling in all of the missing values in the dataset, I use &lt;strong&gt;dplyr&lt;/strong&gt; to group the data according to the day and replace the NA with the average number of steps across all days of its corresponding interval (from the table &lt;strong&gt;MeanInterval&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;I create then a new dataset that is equal to the original dataset but with the missing data filled in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;activityFull &amp;lt;- activity %&amp;gt;% group_by(date) %&amp;gt;% mutate(steps = ifelse(is.na(steps), MeanInterval$x[match(interval, MeanInterval$interval)], steps))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A histogram of the total number of steps taken each day is then re-calculated as well as the mean and median total number of steps taken per day.&lt;/p&gt;
&lt;p&gt;While the new mean is exactly the same as in the first part, the median differs too slightly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StepsPerDayFull &amp;lt;- activityFull %&amp;gt;% group_by(date) %&amp;gt;% summarise(TotSteps = sum(steps))

plot(StepsPerDayFull$date, StepsPerDayFull$TotSteps, type=&amp;quot;h&amp;quot;, lwd=5, col=&amp;quot;red&amp;quot;, 
     xlab=&amp;quot;Days&amp;quot;, 
     ylab=&amp;quot;Number of steps&amp;quot;, 
     main=&amp;quot;Total number of steps taken each day obtained from a full data set&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/activityMonitoring_files/figure-html/histo2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Recap &amp;lt;- xtable(cbind(c(&amp;quot;Without \&amp;quot;NA\&amp;quot;&amp;quot;, &amp;quot;With \&amp;quot;NA\&amp;quot;&amp;quot;), c(mean(StepsPerDayFull$TotSteps), mean(StepsPerDay$TotSteps)), c(median(StepsPerDayFull$TotSteps), median(StepsPerDay$TotSteps))))

colnames(Recap) &amp;lt;- c(&amp;quot;Type of data set&amp;quot;, &amp;quot;Mean&amp;quot;, &amp;quot;Median&amp;quot;)

rownames(Recap) &amp;lt;- NULL

print(Recap, include.rownames=FALSE, type=&amp;quot;html&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;!-- html table generated in R 3.5.1 by xtable 1.8-2 package --&gt;
&lt;!-- Sat Aug 11 00:20:55 2018 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
Type of data set
&lt;/th&gt;
&lt;th&gt;
Mean
&lt;/th&gt;
&lt;th&gt;
Median
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
Without “NA”
&lt;/td&gt;
&lt;td&gt;
10766.1886792453
&lt;/td&gt;
&lt;td&gt;
10766.1886792453
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
With “NA”
&lt;/td&gt;
&lt;td&gt;
10766.1886792453
&lt;/td&gt;
&lt;td&gt;
10765
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;are-there-differences-in-activity-patterns-between-weekdays-and-weekends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Are there differences in activity patterns between weekdays and weekends?&lt;/h2&gt;
&lt;p&gt;I first create a new factor variable in the dataset with two levels “weekday” and “weekend” indicating whether a given date is a weekday or a weekend day.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;activityFull &amp;lt;- activityFull %&amp;gt;% mutate(WE = weekdays(date)) %&amp;gt;% mutate(WE = ifelse(WE == &amp;quot;Samstag&amp;quot;|WE == &amp;quot;Sonntag&amp;quot;, &amp;quot;weekend&amp;quot;, &amp;quot;weekday&amp;quot;))

# With the library &amp;quot;Chron&amp;quot;:
# activityFulltest &amp;lt;- activityFull %&amp;gt;% mutate(WE = ifelse(is.weekend(date), &amp;quot;weekend&amp;quot;, &amp;quot;weekday&amp;quot;)

table(activityFull$WE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## weekday weekend 
##   12960    4608&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I make a panel plot containing a time series plot (i.e. type = “l”) of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all weekday days or weekend days (y-axis).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MeanIntervalFull &amp;lt;- aggregate(activityFull$steps, 
                              by=list(interval=activityFull$interval, WE=activityFull$WE), 
                              FUN = mean)

xyplot(x ~ interval | as.factor(WE), data = MeanIntervalFull, type = &amp;quot;l&amp;quot;, 
       xlab = &amp;quot;Interval&amp;quot;, 
       ylab = &amp;quot;Number of steps&amp;quot;, 
       main = &amp;quot;The average number of steps across all weekday days or weekend days&amp;quot;, 
       layout = c(1,2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/activityMonitoring_files/figure-html/PanelPlot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Evolution of fine particulate matter pollution in the US</title>
      <link>https://slamara.github.io/project/airquality/</link>
      <pubDate>Wed, 08 Aug 2018 16:24:41 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/airquality/</guid>
      <description>&lt;p&gt;Among the numerous air pollutants, the fine particulate matter (PM2.5) is one of the most harmful to human health. In the US, the Environmental Protection Agency (EPA) is in charge of setting the ambient air quality standards, tracking the emissions of the pollutants into the atmosphere, and releasing a database on emissions approximatly every 3 years. This database is known as the National Emissions Inventory (NEI). For more details, see the &lt;a href=&#34;http://www.epa.gov/ttn/chief/eiinformation.html&#34;&gt;EPA National Emissions Inventory web site&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this Database, the NEI records, for each year, how many tons of PM2.5 were emitted. The data used in this project are for 1999, 2002, 2005, and 2008 and can be downloaded &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2FNEI_data.zip&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My goal within this report is to explore the National Emissions Inventory database and see what it says about fine particulate matter pollution in the United states over the 10-year period 1999–2008.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data frame with all of the PM2.5 emissions data for 1999, 2002, 2005, and 2008
NEI &amp;lt;- readRDS(&amp;quot;./airQuality/summarySCC_PM25.rds&amp;quot;)

# mapping from the SCC digit strings in the Emissions table to the actual name of
# the PM2.5 source
SCC &amp;lt;- readRDS(&amp;quot;./airQuality/Source_Classification_Code.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;have-total-emissions-from-pm2.5-decreased-in-the-united-states-from-1999-to-2008&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Have total emissions from PM2.5 decreased in the United States from 1999 to 2008?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;totalEm &amp;lt;- with(NEI, tapply(Emissions, year, sum, na.rm = TRUE))

plot(names(totalEm), totalEm, type =&amp;quot;l&amp;quot;, 
     xlab = &amp;quot;Year&amp;quot;, 
     ylab = &amp;quot;Total Pm(2.5) Emissions (tons)&amp;quot;, 
     main = &amp;quot;Evolution of the total PM2.5 Emissions over the US&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;have-total-emissions-from-pm2.5-decreased-in-the-baltimore-city-maryland-fips24510-from-1999-to-2008&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Have total emissions from PM2.5 decreased in the Baltimore City, Maryland (fips==“24510”) from 1999 to 2008?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NEIBal &amp;lt;- subset(NEI, fips == &amp;quot;24510&amp;quot;)

totalEmBal &amp;lt;- with(NEIBal, tapply(Emissions, year, sum, na.rm = TRUE))

plot(names(totalEmBal), totalEmBal, type =&amp;quot;l&amp;quot;, 
     xlab = &amp;quot;Year&amp;quot;,
     ylab = &amp;quot;Total Pm2.5 Emissions (tons)&amp;quot;,
     main = &amp;quot;Evolution of the total PM2.5 Emissions in Baltimore&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;of-the-four-types-of-sources-indicated-by-the-type-point-nonpoint-onroad-nonroad-variable-which-of-these-four-sources-have-seen-decreases-in-emissions-from-19992008-for-baltimore-city-which-have-seen-increases-in-emissions-from-19992008&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Of the four types of sources indicated by the type (point, nonpoint, onroad, nonroad) variable, which of these four sources have seen decreases in emissions from 1999–2008 for Baltimore City? Which have seen increases in emissions from 1999–2008?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;totalEmBalTypes &amp;lt;- aggregate(NEIBal$Emissions, 
                             by=list(type=NEIBal$type , year=NEIBal$year), 
                             FUN = sum)

with(totalEmBalTypes, qplot(year, x, 
                            color = type, 
                            geom= c(&amp;quot;point&amp;quot;, &amp;quot;line&amp;quot;), 
                            xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;Total PM2.5 Emissions (tons)&amp;quot;, 
                            main = &amp;quot;Evolution of total Emissions in Baltimore by type of source&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;across-the-united-states-how-have-emissions-from-coal-combustion-related-sources-changed-from-19992008&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Across the United States, how have emissions from coal combustion-related sources changed from 1999–2008?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SCCIndex &amp;lt;- as.vector(SCC$SCC[grep(&amp;quot;*comb.*coal&amp;quot;, SCC$Short.Name, ignore.case = TRUE)])

NEICoal &amp;lt;- subset(NEI, SCC %in% SCCIndex)

totalCoal &amp;lt;- with(NEICoal, tapply(Emissions, year, sum, na.rm = TRUE))

plot(names(totalCoal), totalCoal, type =&amp;quot;l&amp;quot;, 
     xlab = &amp;quot;Year&amp;quot;, 
     ylab = &amp;quot;Total Pm2.5 Emissions (tons)&amp;quot;, 
     main = &amp;quot;Total Emission from coal combustion-related sources over the US  &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-have-emissions-from-motor-vehicle-sources-changed-from-19992008-in-baltimore-city&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How have emissions from motor vehicle sources changed from 1999–2008 in Baltimore City?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NEIBal &amp;lt;- subset(NEI, fips == &amp;quot;24510&amp;quot; &amp;amp; type == &amp;quot;ON-ROAD&amp;quot;)

totalBalMotor &amp;lt;- with(NEIBal, tapply(Emissions, year, sum, na.rm = TRUE))

plot(names(totalBalMotor), totalBalMotor, type =&amp;quot;l&amp;quot;, 
     xlab = &amp;quot;Year&amp;quot;, 
     ylab = &amp;quot;Total Pm2.5 Emissions (tons)&amp;quot;, 
     main = &amp;quot;Total Emission from Motor Vehicles in Baltimore City&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-emissions-from-motor-vehicle-sources-in-baltimore-city-with-emissions-from-motor-vehicle-sources-in-los-angeles-county-california-fips06037.-which-city-has-seen-greater-changes-over-time-in-motor-vehicle-emissions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Compare emissions from motor vehicle sources in Baltimore City with emissions from motor vehicle sources in Los Angeles County, California (fips==“06037”). Which city has seen greater changes over time in motor vehicle emissions?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NEIComp &amp;lt;- subset(NEI, fips %in% c(&amp;quot;24510&amp;quot;, &amp;quot;06037&amp;quot;) &amp;amp; type == &amp;quot;ON-ROAD&amp;quot;)

totalComp &amp;lt;- aggregate(NEIComp$Emissions, 
                       by=list(fips= NEIComp$fips, type=NEIComp$type, year=NEIComp$year), 
                       FUN = sum)

with(totalComp, qplot(year, x, color = fips, geom= c(&amp;quot;point&amp;quot;, &amp;quot;line&amp;quot;), 
                      xlab = &amp;quot;Year&amp;quot;, 
                      ylab = &amp;quot;Total PM2.5 Emissions (tons)&amp;quot;, 
                      main = &amp;quot;Evolution of total Emissions from Motor Vehicles in Baltimore city \n      (fips = 24510) and Los Angeles County (fips = 06037)&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Individual household electric power consumption</title>
      <link>https://slamara.github.io/project/electricpconsumption/</link>
      <pubDate>Wed, 08 Aug 2018 16:08:18 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/electricpconsumption/</guid>
      <description>&lt;p&gt;The goal of this project is to examine how household energy usage varies over a 2-day period in February, 2007 using the base plotting system. The &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2Fhousehold_power_consumption.zip&#34;&gt;data&lt;/a&gt; used in here (in particular, the “Individual household electric power consumption Data Set”) are from the &lt;a href=&#34;http://archive.ics.uci.edu/ml/&#34;&gt;UC Irvine Machine Learning Repository&lt;/a&gt;, a popular repository for machine learning datasets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sqldf)
library(dplyr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Estimate the memory usage
paste0(&amp;quot;As the dataset has 2880 rows and 9 columns, it requires about &amp;quot;, 
           round(2880 * 9 * 8 / 2^20, 2), &amp;quot; Megabytes in memory.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;As the dataset has 2880 rows and 9 columns, it requires about 0.2 Megabytes in memory.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read the data using an SQL statement
sel_data &amp;lt;- read.csv.sql(&amp;quot;./electricPConsumption/household_power_consumption.txt&amp;quot;, 
                            sep = &amp;#39;;&amp;#39;, header = TRUE, 
                            sql=&amp;quot;select * from file where Date in (&amp;#39;1/2/2007&amp;#39;, &amp;#39;2/2/2007&amp;#39;)&amp;quot;)

paste0(&amp;quot;Used memory: &amp;quot;, format(object.size(sel_data), units = &amp;quot;Mb&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Used memory: 0.3 Mb&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a datetime column given a specific format
sel_data &amp;lt;- sel_data %&amp;gt;% mutate(datetime = as.POSIXct(paste(sel_data$Date, sel_data$Time), 
                                                          format=&amp;quot;%d/%m/%Y %H:%M:%S&amp;quot;))

# Plot the histogram
hist(sel_data$Global_active_power, col = &amp;quot;red&amp;quot;, main = &amp;quot;Global Active Power&amp;quot;, 
         xlab = &amp;quot;Global Active Power (kilowatts)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/electricPConsumption_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a png file from the histogram
png(file = &amp;quot;plot1.png&amp;quot;, width = 480, height = 480, bg = &amp;quot;transparent&amp;quot;)

hist(sel_data$Global_active_power, col = &amp;quot;red&amp;quot;, main = &amp;quot;Global Active Power&amp;quot;, 
         xlab = &amp;quot;Global Active Power (kilowatts)&amp;quot;)

dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(sel_data$datetime, sel_data$Global_active_power, type = &amp;quot;l&amp;quot;, xlab = &amp;quot;&amp;quot;, 
         ylab = &amp;quot;Global Active Power (kilowatts)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/electricPConsumption_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(2, 2))

plot(sel_data$datetime, sel_data$Global_active_power, type = &amp;quot;l&amp;quot;, xlab = &amp;quot;&amp;quot;, 
         ylab = &amp;quot;Global Active Power&amp;quot;)

plot(sel_data$datetime, sel_data$Voltage, type = &amp;quot;l&amp;quot;, xlab = &amp;quot;datetime&amp;quot;, 
         ylab = &amp;quot;Voltage&amp;quot;)

plot(sel_data$datetime, sel_data$Sub_metering_1, type = &amp;quot;l&amp;quot;, 
         ylim = range(sel_data$Sub_metering_1), xlab = &amp;quot;&amp;quot;, ylab= &amp;quot;Energy sub metering&amp;quot;)

par(new = TRUE)

plot(sel_data$datetime, sel_data$Sub_metering_2, type = &amp;quot;l&amp;quot;, 
         ylim = range(sel_data$Sub_metering_1), col = &amp;quot;red&amp;quot;, 
         xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, xlab = &amp;quot;&amp;quot;, ylab= &amp;quot;&amp;quot;)

par(new = TRUE)

plot(sel_data$datetime, sel_data$Sub_metering_3, type = &amp;quot;l&amp;quot;, 
         ylim = range(sel_data$Sub_metering_1), col = &amp;quot;blue&amp;quot;, 
         xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, xlab = &amp;quot;&amp;quot;, ylab= &amp;quot;&amp;quot;)

legend(&amp;quot;topright&amp;quot;,legend=c(&amp;quot;Sub_metering_1&amp;quot;, &amp;quot;Sub_metering_2&amp;quot;, &amp;quot;Sub_metering_3&amp;quot;), 
           bty = &amp;quot;n&amp;quot;, lty = c(1,1),col=c(&amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;))

plot(sel_data$datetime, sel_data$Global_reactive_power, type = &amp;quot;l&amp;quot;, 
         xlab = &amp;quot;datetime&amp;quot;, ylab = &amp;quot;Global_reactive_power&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/electricPConsumption_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting and cleaning data</title>
      <link>https://slamara.github.io/project/gcd/</link>
      <pubDate>Fri, 03 Aug 2018 18:05:56 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/gcd/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The purpose of this project is to show the ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis.&lt;/p&gt;
&lt;p&gt;The data used in this project represent data collected from the accelerometers of Samsung Galaxy S smartphones. For a full description please visit the following website:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones&#34; class=&#34;uri&#34;&gt;http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones&lt;/a&gt; &lt;sup&gt;&lt;a href=&#34;#myfootnote1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Here are the data for the project:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip&#34;&gt;https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The script does the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load the libraries &lt;em&gt;data.table&lt;/em&gt; and &lt;em&gt;dplyr&lt;/em&gt; needed to run the script&lt;/li&gt;
&lt;li&gt;Read the files &lt;em&gt;subject_test.txt&lt;/em&gt;, &lt;em&gt;y_test.txt&lt;/em&gt;, &lt;em&gt;X_test.txt&lt;/em&gt; contained in folder &lt;strong&gt;test&lt;/strong&gt; as well as the files &lt;em&gt;subject_train.txt&lt;/em&gt;, &lt;em&gt;y_train.txt&lt;/em&gt;, &lt;em&gt;X_train.txt&lt;/em&gt; in folder &lt;strong&gt;train&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Extract features and activity names to Label the data sets with descriptive variable names&lt;/li&gt;
&lt;li&gt;Merge the training and the tests sets to create one data set&lt;/li&gt;
&lt;li&gt;Coerce the column names to obtain syntactically valid ones&lt;/li&gt;
&lt;li&gt;Extract only the measurements on the mean and standard deviation for each measurement (&lt;strong&gt;sel_data&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Use descriptive activity names to name the activities in the selected data set&lt;/li&gt;
&lt;li&gt;Create a second independent tidy data set with the average of each variable for each activity and each subject (&lt;strong&gt;mean_data&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Create the output files &lt;em&gt;sel_data.csv&lt;/em&gt; and &lt;em&gt;mean_data.csv&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Re-initialize the Global Environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries

library(data.table)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read Dataset

subject_test &amp;lt;- fread(&amp;quot;./gcd/subject_test.txt&amp;quot;)
y_test &amp;lt;- fread(&amp;quot;./gcd/y_test.txt&amp;quot;)
x_test &amp;lt;- fread(&amp;quot;./gcd/X_test.txt&amp;quot;)

subject_train &amp;lt;- fread(&amp;quot;./gcd/subject_train.txt&amp;quot;)
y_train &amp;lt;- fread(&amp;quot;./gcd/y_train.txt&amp;quot;)
x_train &amp;lt;- fread(&amp;quot;./gcd/X_train.txt&amp;quot;)

# Extract features and activity names

cnames &amp;lt;- fread(&amp;quot;./gcd/features.txt&amp;quot;)

lActivities &amp;lt;- fread(&amp;quot;./gcd/activity_labels.txt&amp;quot;)

# Label the data sets with descriptive variable names

colnames(y_test) &amp;lt;- &amp;quot;activity&amp;quot;
colnames(subject_test) &amp;lt;- &amp;quot;subject&amp;quot;
colnames(x_test) &amp;lt;- as.character(cnames$V2)
all_test &amp;lt;- cbind(subject_test, y_test, x_test)

colnames(y_train) &amp;lt;- &amp;quot;activity&amp;quot;
colnames(subject_train) &amp;lt;- &amp;quot;subject&amp;quot;
colnames(x_train) &amp;lt;- as.character(cnames$V2)
all_train &amp;lt;- cbind(subject_train, y_train, x_train)


# Merge the training and the tests sets to create one data set

all_data &amp;lt;- rbind(all_test, all_train)
paste0(&amp;quot;Number of variables: &amp;quot;, dim(all_data)[2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of variables: 563&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste0(&amp;quot;Number of Observations: &amp;quot;, dim(all_data)[1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of Observations: 10299&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Coerce column names to obtain syntactically valid ones

valid_names &amp;lt;- make.names(names=names(all_data), unique=TRUE, allow_ = TRUE)
names(all_data) &amp;lt;- valid_names

# Extract only the measurements on the mean and standard deviation for each measurement

sel_data &amp;lt;- select(all_data, matches(&amp;quot;subject|activity|\\.mean\\.|\\.std\\.&amp;quot;))

names(sel_data) &amp;lt;- gsub(names(sel_data), pattern = &amp;quot;\\.\\.&amp;quot;, replacement = &amp;quot;&amp;quot;)

# Use descriptive activity names to name the activities in the selected data set

sel_data$activity &amp;lt;- lActivities$V2[match(sel_data$activity, lActivities$V1)]

paste0(&amp;quot;The recorded activities are: &amp;quot;, paste(unique(sel_data$activity), collapse = &amp;quot;, &amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;The recorded activities are: STANDING, SITTING, LAYING, WALKING, WALKING_DOWNSTAIRS, WALKING_UPSTAIRS&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.table(sel_data, file = &amp;quot;./gcd/sel_data.txt&amp;quot;, row.names = FALSE)

# From the previous data set, create a second independent tidy data set with the average
# of each variable for each activity and each subject

mean_data &amp;lt;- sel_data %&amp;gt;% group_by(subject, activity) %&amp;gt;% summarise_all(funs(mean))

write.table(mean_data, file = &amp;quot;./gcd/mean_data.txt&amp;quot;, row.names = FALSE)

# Re-initialize the Global Environment

rm(list = ls())&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;p&gt;&lt;a name=&#34;myfootnote1&#34;&gt;[1]&lt;/a&gt;: Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ranking US Hospitals</title>
      <link>https://slamara.github.io/project/rankingushospitals/</link>
      <pubDate>Fri, 03 Aug 2018 18:05:56 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/rankingushospitals/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The purpose of this project is to rank over 4000 US hospitals according to the quality of care. The data represent a small subset of the data available at the Hospital Compare web site (&lt;a href=&#34;http://hospitalcompare.hhs.gov&#34; class=&#34;uri&#34;&gt;http://hospitalcompare.hhs.gov&lt;/a&gt;) run by the U.S. Department of Health and Human Services.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/rprog%2Fdata%2FProgAssignment3-data.zip&#34;&gt;zip file&lt;/a&gt; for this project contains three files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;outcome-of-care-measures.csv&lt;/strong&gt;: Contains information about 30-day mortality and readmission rates for heart attacks, heart failure, and pneumonia for over 4,000 hospitals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hospital-data.csv&lt;/strong&gt;: Contains information about each hospital.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hospital_Revised_Flatfiles.pdf&lt;/strong&gt;: Descriptions of the variables in each file (i.e the code book). This document contains information about many other files that are not included with this project. We want to focus on the variables for Number 19 (“Outcome of Care Measures.csv”) and Number 11 (“Hospital Data.csv”).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;finding-the-best-hospital-in-a-state&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Finding the best hospital in a state&lt;/h2&gt;
&lt;p&gt;I write a function called &lt;strong&gt;best&lt;/strong&gt; which takes two arguments: the 2-character abbreviated name of a state and an outcome name. The function returns a character vector with the name of the hospital that has the lowest 30-day mortality for the specified outcome in that state. The outcomes can be one of “heart attack”, “heart failure”, or “pneumonia”. The Hospitals that do not have data on a particular outcome are excluded from the set of hospitals when deciding the rankings.&lt;/p&gt;
&lt;p&gt;If there is a tie for the best hospital for a given outcome, then the hospital names should be sorted in alphabetical order and the first hospital in that set should be chosen.&lt;/p&gt;
&lt;p&gt;The function checks the validity of its arguments and throws an error via the stop function with the message “invalid state” or “invalid outcome” when an invalid state resp. outcome value is passed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best &amp;lt;- function(state, outcome) {
## Read outcome data

    data &amp;lt;- read.csv(&amp;quot;./rankingUsHospitals/outcome-of-care-measures.csv&amp;quot;, 
                         colClasses = &amp;quot;character&amp;quot;)

## Check that state and outcome are valid

    if (!(state %in% data$State)) {
        result &amp;lt;- &amp;quot;invalid state&amp;quot;
      }
    else if (!outcome %in% c(&amp;quot;heart attack&amp;quot;, &amp;quot;heart failure&amp;quot;, &amp;quot;pneumonia&amp;quot;)) {
        result &amp;lt;- &amp;quot;invalid outcome&amp;quot;
      }
    else{
        keys &amp;lt;- c(&amp;quot;heart attack&amp;quot; = 11, &amp;quot;heart failure&amp;quot; = 17, &amp;quot;pneumonia&amp;quot; = 23)
        outcomeKey &amp;lt;- keys[outcome]
  
## Return hospital name in that state with lowest 30-day death rate
  
        dataPerState &amp;lt;- split(data, data$State)
        dataOurState &amp;lt;- dataPerState[[state]]
        dataOurState &amp;lt;- dataOurState[ order(dataOurState[&amp;quot;Hospital.Name&amp;quot;]), ]
        dataOutcome &amp;lt;- suppressWarnings(as.numeric(dataOurState[, outcomeKey]))
        good &amp;lt;- complete.cases(dataOutcome)
        dataOutcome &amp;lt;- dataOutcome[good]
        dataOurState &amp;lt;- dataOurState[good,]
        minimum &amp;lt;- min(dataOutcome)
        index &amp;lt;- match(minimum, dataOutcome)
        result &amp;lt;- dataOurState[index, 2]
    }
    result
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;testing-best&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing &lt;strong&gt;best&lt;/strong&gt;:&lt;/h3&gt;
&lt;p&gt;A set of state names and outcomes is used to check the function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chk1 &amp;lt;- c(&amp;quot;TX&amp;quot;, &amp;quot;heart attack&amp;quot;)
chk2 &amp;lt;- c(&amp;quot;TX&amp;quot;, &amp;quot;heart failure&amp;quot;)
chk3 &amp;lt;- c(&amp;quot;MD&amp;quot;, &amp;quot;heart attack&amp;quot;)
chk4 &amp;lt;- c(&amp;quot;MD&amp;quot;, &amp;quot;pneumonia&amp;quot;)
chk5 &amp;lt;- c(&amp;quot;BB&amp;quot;, &amp;quot;heart attack&amp;quot;)
chk6 &amp;lt;- c(&amp;quot;NY&amp;quot;, &amp;quot;hert attack&amp;quot;)
dat &amp;lt;- data.table(chk1, chk2, chk3, chk4, chk5, chk6)
dat &amp;lt;- t(dat)
as.list(apply(dat, 1, function(x){do.call(best, as.list(x))}))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $chk1
## [1] &amp;quot;CYPRESS FAIRBANKS MEDICAL CENTER&amp;quot;
## 
## $chk2
## [1] &amp;quot;FORT DUNCAN MEDICAL CENTER&amp;quot;
## 
## $chk3
## [1] &amp;quot;JOHNS HOPKINS HOSPITAL, THE&amp;quot;
## 
## $chk4
## [1] &amp;quot;GREATER BALTIMORE MEDICAL CENTER&amp;quot;
## 
## $chk5
## [1] &amp;quot;invalid state&amp;quot;
## 
## $chk6
## [1] &amp;quot;invalid outcome&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ranking-hospitals-by-outcome-in-a-state&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ranking hospitals by outcome in a state&lt;/h2&gt;
&lt;p&gt;To this intent, I write a function &lt;strong&gt;rankHospital&lt;/strong&gt; which takes three arguments: the 2-character abbreviated name of a state (state), an outcome (outcome), and the ranking of a hospital in that state for that outcome (num).&lt;/p&gt;
&lt;p&gt;The function returns a character vector with the name of the hospital that has the ranking specified by the &lt;strong&gt;num&lt;/strong&gt; argument. The &lt;strong&gt;num&lt;/strong&gt; argument can take the values “best”, “worst”, or an integer indicating the ranking.&lt;/p&gt;
&lt;p&gt;The Hospitals that do not have data on a particular outcome are excluded from the set of hospitals when deciding the rankings. Also, If the number given by num is larger than the number of hospitals in that state, then the function returns NA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rankHospital &amp;lt;- function(state, outcome, num = &amp;quot;best&amp;quot;) {
    
  
## Read outcome data

    data &amp;lt;- read.csv(&amp;quot;./rankingUsHospitals/outcome-of-care-measures.csv&amp;quot;, 
                         colClasses = &amp;quot;character&amp;quot;)

## Check that state and outcome are valid

    if (!(state %in% data$State)) {
        result &amp;lt;- &amp;quot;invalid state&amp;quot;
    }
    else if (!outcome %in% c(&amp;quot;heart attack&amp;quot;, &amp;quot;heart failure&amp;quot;, &amp;quot;pneumonia&amp;quot;)) {
        result &amp;lt;- &amp;quot;invalid outcome&amp;quot;
    }
    else {
        keys &amp;lt;- c(&amp;quot;heart attack&amp;quot; = 11, &amp;quot;heart failure&amp;quot; = 17, &amp;quot;pneumonia&amp;quot; = 23)
        outcomeKey &amp;lt;- keys[outcome]
  
  
## Return hospital name in that state with the given rank
## 30-day death rate
  
        dataPerState &amp;lt;- split(data, data$State)
        dataOurState &amp;lt;- dataPerState[[state]]
        dataOutcome &amp;lt;- suppressWarnings(as.numeric(dataOurState[, outcomeKey]))
        good &amp;lt;- complete.cases(dataOutcome)
        dataOutcome &amp;lt;- dataOutcome[good]
        dataOurState &amp;lt;- dataOurState[good,]
        dataOurState &amp;lt;- dataOurState[order(dataOutcome, dataOurState[&amp;quot;Hospital.Name&amp;quot;]),]
        if (grepl(&amp;quot;^[0-9]+$&amp;quot;, num)) {
            if (as.numeric(num) &amp;gt; length(dataOutcome)) {
                result &amp;lt;- NA
            }
            else {
                result &amp;lt;- dataOurState[as.numeric(num), &amp;quot;Hospital.Name&amp;quot;]
            }
        }    
        else if (num == &amp;quot;best&amp;quot;) {
                result &amp;lt;- dataOurState[1, &amp;quot;Hospital.Name&amp;quot;]
        }
        else if (num == &amp;quot;worst&amp;quot;) {
                result &amp;lt;- dataOurState[length(dataOutcome), &amp;quot;Hospital.Name&amp;quot;]
        }
        else result &amp;lt;- NA
    }
    result
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;testing-rankhospital&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing &lt;strong&gt;rankHospital&lt;/strong&gt;&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chk1 &amp;lt;- c(&amp;quot;TX&amp;quot;, &amp;quot;heart failure&amp;quot;, 4)
chk2 &amp;lt;- c(&amp;quot;MD&amp;quot;, &amp;quot;heart attack&amp;quot;, &amp;quot;worst&amp;quot;)
chk3 &amp;lt;- c(&amp;quot;MN&amp;quot;, &amp;quot;heart attack&amp;quot;, 5000)
dat &amp;lt;- data.table(chk1, chk2, chk3)
dat &amp;lt;- t(dat)
as.list(apply(dat, 1, function(x){do.call(rankHospital, as.list(x))}))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $chk1
## [1] &amp;quot;DETAR HOSPITAL NAVARRO&amp;quot;
## 
## $chk2
## [1] &amp;quot;HARFORD MEMORIAL HOSPITAL&amp;quot;
## 
## $chk3
## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ranking-hospitals-in-all-states&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ranking hospitals in all states&lt;/h2&gt;
&lt;p&gt;I implement a function &lt;strong&gt;rankAll&lt;/strong&gt; which takes as arguments the outcome name (outcome) and hospital ranking (num) and returns a 2-column data frame containing the hospital in each state that has the ranking specified in num.&lt;/p&gt;
&lt;p&gt;The function returns a value for every state (some may be NA). The first column in the data frame contains the hospital name and the second one contains the 2-character abbreviation for the state name. Hospitals that do not have data on a particular outcome are excluded from the set of hospitals when deciding the rankings.&lt;/p&gt;
&lt;p&gt;Although it is possible to call the &lt;strong&gt;rankHospital&lt;/strong&gt; function from the previous section, I decided, for didactic purposes, not using it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rankAll &amp;lt;- function(outcome, num = &amp;quot;best&amp;quot;) {

    dataAll &amp;lt;- data.frame(hospital = character(), state = character())
  
## Read outcome data

    data &amp;lt;- read.csv(&amp;quot;./rankingUsHospitals/outcome-of-care-measures.csv&amp;quot;, 
                         colClasses = &amp;quot;character&amp;quot;)
  
## Check that outcome and num are valid

    if (!outcome %in% c(&amp;quot;heart attack&amp;quot;, &amp;quot;heart failure&amp;quot;, &amp;quot;pneumonia&amp;quot;)) {
        dataAll &amp;lt;- &amp;quot;invalid outcome&amp;quot;
    }
    else {
        keys &amp;lt;- c(&amp;quot;heart attack&amp;quot; = 11, &amp;quot;heart failure&amp;quot; = 17, &amp;quot;pneumonia&amp;quot; = 23)
        outcomeKey &amp;lt;- keys[outcome]

## For each state, find the hospital of the given rank

        dataPerState &amp;lt;- split(data, data$State)
        for (stat in names(dataPerState)) {
        dataOurState &amp;lt;- dataPerState[[stat]]
        dataOutcome &amp;lt;- suppressWarnings(as.numeric(dataOurState[, outcomeKey]))
        good &amp;lt;- complete.cases(dataOutcome)
        dataOutcome &amp;lt;- dataOutcome[good]
        dataOurState &amp;lt;- dataOurState[good,]
        dataOurState &amp;lt;- dataOurState[ order(dataOutcome, dataOurState[&amp;quot;Hospital.Name&amp;quot;]), ]
        
        if (num == &amp;quot;best&amp;quot;) {
            numState &amp;lt;- c(1)
        } else {
            if (num == &amp;quot;worst&amp;quot;) {
                numState &amp;lt;- length(dataOutcome)
            } else {
                numState &amp;lt;- num
            }
        }
    
        dataPart &amp;lt;- data.frame(hospital = dataOurState[numState, &amp;quot;Hospital.Name&amp;quot;], 
                                   state = stat, row.names = stat)
        
        dataAll &amp;lt;- rbind(dataAll, dataPart)
        }
    }

## Return a data frame with the hospital names and the (abbreviated) state name

    dataAll
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;testing-rankall&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing &lt;strong&gt;rankAll&lt;/strong&gt;&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(rankAll(&amp;quot;heart attack&amp;quot;, 20), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                               hospital state
## AK                                &amp;lt;NA&amp;gt;    AK
## AL      D W MCMILLAN MEMORIAL HOSPITAL    AL
## AR   ARKANSAS METHODIST MEDICAL CENTER    AR
## AZ JOHN C LINCOLN DEER VALLEY HOSPITAL    AZ
## CA               SHERMAN OAKS HOSPITAL    CA
## CO            SKY RIDGE MEDICAL CENTER    CO
## CT             MIDSTATE MEDICAL CENTER    CT
## DC                                &amp;lt;NA&amp;gt;    DC
## DE                                &amp;lt;NA&amp;gt;    DE
## FL      SOUTH FLORIDA BAPTIST HOSPITAL    FL&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(rankAll(&amp;quot;pneumonia&amp;quot;, &amp;quot;worst&amp;quot;), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                      hospital state
## WI MAYO CLINIC HEALTH SYSTEM - NORTHLAND, INC    WI
## WV                     PLATEAU MEDICAL CENTER    WV
## WY           NORTH BIG HORN HOSPITAL DISTRICT    WY&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(rankAll(&amp;quot;heart failure&amp;quot;), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                             hospital state
## TN                         WELLMONT HAWKINS COUNTY MEMORIAL HOSPITAL    TN
## TX                                        FORT DUNCAN MEDICAL CENTER    TX
## UT VA SALT LAKE CITY HEALTHCARE - GEORGE E. WAHLEN VA MEDICAL CENTER    UT
## VA                                          SENTARA POTOMAC HOSPITAL    VA
## VI                            GOV JUAN F LUIS HOSPITAL &amp;amp; MEDICAL CTR    VI
## VT                                              SPRINGFIELD HOSPITAL    VT
## WA                                         HARBORVIEW MEDICAL CENTER    WA
## WI                                    AURORA ST LUKES MEDICAL CENTER    WI
## WV                                         FAIRMONT GENERAL HOSPITAL    WV
## WY                                        CHEYENNE VA MEDICAL CENTER    WY&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>https://slamara.github.io/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>https://slamara.github.io/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
