<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Samir Lamara on Samir Lamara</title>
    <link>https://slamara.github.io/</link>
    <description>Recent content in Samir Lamara on Samir Lamara</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Titanic: Machine Learning from Disaster- A Kaggle Competition</title>
      <link>https://slamara.github.io/project/titanic/</link>
      <pubDate>Wed, 08 Aug 2018 17:42:32 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/titanic/</guid>
      <description>&lt;p&gt;hello!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting correctness of weight lifting exercises</title>
      <link>https://slamara.github.io/project/weightlifting/</link>
      <pubDate>Wed, 08 Aug 2018 17:17:52 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/weightlifting/</guid>
      <description>&lt;div id=&#34;background&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;Many fitness enthusiasts focuse mainly on how often they practice and how heavy they can lift, and usually neglect to verify whether they do it well or not. Yet, injuries due to unnatural movements and overexertion were proved to be the most commonly occuring ones during free weight activities &lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. With the increasing use of devices that are equipped with accelerometers it is now possible to quantify how well a movement is done.&lt;/p&gt;
&lt;p&gt;In this project, I use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to a scientific study. During this experiment, the participants were asked to perform barbell lifts in 5 different ways: one correct and four incorrect. More information is available from the website &lt;a href=&#34;http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har&#34;&gt;here&lt;/a&gt;. See the section on the Weight Lifting Exercise Dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;The training data for this project is available here: &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&#34;&gt;pml-training.csv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The test data is available here: &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&#34;&gt;pml-testing.csv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For reproducibility, the following libraries should be installed and loaded in order to accomplish all processing steps included in this report:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
library(rattle)
library(parallel)
library(doParallel)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Processing&lt;/h3&gt;
&lt;p&gt;When it comes to machine learning, one should consider the quality of data as most crucial element. Incomplete, irrelevant and inaccurate data sets are all sources of errors that will be inevitably incorporated in any machine learning analysis, giving to the observation &lt;em&gt;“garbage in, garbage out”&lt;/em&gt; all its sense. For such high-quality demanding analysis, one should thus spend enough time wrangling data efficiently.&lt;br /&gt;
Fortunately, the tidying of the data was already done &lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; (a data summary is given &lt;a href=&#34;https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-predictionSummary.md&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Nonetheless, a look at the data content shows at first sight a substantive occurrence of various missing and/or irrelevant inputs (“NA”, “”, #Div/0!“) which need to be normalized from the notational point of view.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#download.file(&amp;quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&amp;quot;, &amp;quot;pml-training.csv&amp;quot;)
#download.file(&amp;quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&amp;quot;, &amp;quot;pml-testing.csv&amp;quot;)
preTest &amp;lt;- read.csv(&amp;quot;./weightLifting/pml-testing.csv&amp;quot;, na.strings = c(&amp;quot;NA&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;#DIV/0!&amp;quot;))
preTrain &amp;lt;- read.csv(&amp;quot;./weightLifting/pml-training.csv&amp;quot;, na.strings = c(&amp;quot;NA&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;#DIV/0!&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is then processed as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;the 1st and 6th columns (resp. observation’s number and “new_number”) are irrelevant in this context and are excluded,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the columns with down to 90% of “NA” values are also excluded,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In case, two predictors present high pairwise correlation (in term of the Pearson’s correlation coefficient &lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;), only the best is kept (the one with the smallest mean absolute correlation). The cut-off value of the Pearson’s correlation coefficient is usually chosen to be higher than 0.75. Since we use variables recorded by body sensors that could be mutually influenced, I choose a bit higher value of 0.8,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the predictors with near-zero variance should be excluded. However, all of them were already excluded within the previous steps,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;we subset the training data into training and testing sets and set up the training run with the (x, y) syntax.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1. Exclude the 1st and 6th columns
preTest &amp;lt;- preTest[, -c(1, 6)] 
preTrain &amp;lt;- preTrain[, -c(1, 6)]

# 2. Exclude the columns with too many NAs (cut off at 90%)
training &amp;lt;- preTrain[,colSums(is.na(preTrain))/nrow(preTrain) &amp;lt; 0.9]
cIndex &amp;lt;- which(!(colnames(preTrain) %in% colnames(training)))
testing &amp;lt;- preTest[, - cIndex]

# 3. Exclude the predictors that are highly correlated
analyseDataTrain &amp;lt;- sapply(training[,-58], as.numeric) # column 58 is the target variable
corMat &amp;lt;- cor(analyseDataTrain)
highCor &amp;lt;- findCorrelation(corMat, cutoff=0.8)
trainingCor &amp;lt;- training[, -highCor]
testingCor &amp;lt;- testing[, -highCor]

# 4. Exclude the predictors with very small variance
#nsv &amp;lt;- nearZeroVar(trainingCor) # in this case nsv is empty

# 5. Subset the data
set.seed(123)
subsets &amp;lt;- createDataPartition(y=trainingCor$classe, p=0.75, list=FALSE)
subTrainingCor &amp;lt;- trainingCor[subsets, ] 
subTestingCor &amp;lt;- trainingCor[-subsets, ]
x &amp;lt;- subTrainingCor[, -44] # column 44 is the target variable
y &amp;lt;- subTrainingCor[, 44]
rm(preTest, preTrain,testing, training, cIndex, analyseDataTrain, corMat, highCor, subsets)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;computer-configuration-used-in-this-project&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Computer configuration used in this project&lt;/h3&gt;
&lt;table style=&#34;width:100%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;24%&#34; /&gt;
&lt;col width=&#34;75%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Computer&lt;/th&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Samsung Series 5 Ultra&lt;/td&gt;
&lt;td&gt;* Operating system: Windows 10 (64 Bits)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;* Processor: Intel Corei5 3337U @ 1.80GHz up to 2.7GHz (2 cores, 4 threads)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;* RAM: 8 Gb&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;* Disk: 512 Gb SSD&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-up-the-parallel-environment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting up the parallel environment&lt;/h3&gt;
&lt;p&gt;To improve processing time I decided to use caret on parallel environment. This accomplished as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cluster &amp;lt;- makeCluster(detectCores() - 1)  # 1 core is left for OS
registerDoParallel(cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model fitting&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;trainControl&lt;/strong&gt; was set to use the K-fold cross-validation as it represents a robust method to estimate the model’s accuracy. The choice of k = 5 has been empirically shown to avoid high bias and variance when estimating the test error rate &lt;a href=&#34;#fn4&#34; class=&#34;footnoteRef&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this experiment, 6 participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;exactly according to the specification (Class A)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;throwing the elbows to the front (Class B)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;lifting the dumbbell only halfway (Class C)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;lowering the dumbbell only halfway (Class D)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;throwing the hips to the front (Class E).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the class A corresponds to the specified execution of the exercise, the other 4 classes correspond to common mistakes.&lt;/p&gt;
&lt;p&gt;As we try to predict these behaviors through predictor variables, the &lt;strong&gt;target variable&lt;/strong&gt;, “classe”, is thus a factor with 5 different levels corresponding to the five preceding cases.&lt;/p&gt;
&lt;p&gt;To illustrate the difference in accuracy according to the method used, three different machine learning methods are chosen:&lt;/p&gt;
&lt;div id=&#34;decision-tree&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1. Decision tree&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
control = trainControl(method = &amp;quot;cv&amp;quot;, number = 5, allowParallel = TRUE)
system.time(modelRPART &amp;lt;- train(x, y, method = &amp;quot;rpart&amp;quot;, trControl = control)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        User      System verstrichen 
##        6.00        0.12       20.12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modelRPART$results&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           cp  Accuracy     Kappa  AccuracySD    KappaSD
## 1 0.03398842 0.5565960 0.4378718 0.009052306 0.01130354
## 2 0.03792842 0.5119644 0.3733120 0.036654830 0.05876857
## 3 0.06728852 0.3606495 0.1255237 0.104492903 0.17188225&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predRPART &amp;lt;- predict(modelRPART, subTestingCor)
cfMatRPART &amp;lt;- confusionMatrix(subTestingCor$classe, predRPART)
cfMatRPART$overall&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##   5.309951e-01   4.053248e-01   5.169111e-01   5.450422e-01   4.783850e-01 
## AccuracyPValue  McnemarPValue 
##   9.423710e-14   0.000000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cfMatRPART$table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Reference
## Prediction    A    B    C    D    E
##          A 1180   31  184    0    0
##          B  158  317  472    0    2
##          C   16   32  806    0    1
##          D   71  152  521    0   60
##          E   69  168  363    0  301&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fancyRpartPlot(modelRPART$finalModel)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/weightLifting_files/figure-html/ModelPredRPART-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model obtained has a low accuracy of 55,66% and cannot be used to predict the results of the test data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-boosting-method&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2. Gradient boosting method&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123) 
elapsed &amp;lt;- system.time(modelGBM &amp;lt;- train(x, y, method = &amp;quot;gbm&amp;quot;, trControl = control)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        User      System verstrichen 
##       33.14        0.16      172.27&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modelGBM$results&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   shrinkage interaction.depth n.minobsinnode n.trees  Accuracy     Kappa
## 1       0.1                 1             10      50 0.7842776 0.7262820
## 4       0.1                 2             10      50 0.9287265 0.9097723
## 7       0.1                 3             10      50 0.9660959 0.9570896
## 2       0.1                 1             10     100 0.8660808 0.8303878
## 5       0.1                 2             10     100 0.9807720 0.9756724
## 8       0.1                 3             10     100 0.9938168 0.9921793
## 3       0.1                 1             10     150 0.9033830 0.8776612
## 6       0.1                 2             10     150 0.9924579 0.9904598
## 9       0.1                 3             10     150 0.9974179 0.9967340
##    AccuracySD     KappaSD
## 1 0.007445123 0.009479860
## 4 0.005811340 0.007363580
## 7 0.002180500 0.002753041
## 2 0.008072280 0.010331460
## 5 0.001602403 0.002032181
## 8 0.001883989 0.002383054
## 3 0.008613351 0.010989106
## 6 0.002235010 0.002827177
## 9 0.001658198 0.002097462&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predGBM &amp;lt;- predict(modelGBM, subTestingCor) 
cfMatGBM &amp;lt;- confusionMatrix(subTestingCor$classe, predGBM)
cfMatGBM$overall&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##      0.9977569      0.9971627      0.9959901      0.9988798      0.2846656 
## AccuracyPValue  McnemarPValue 
##      0.0000000            NaN&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cfMatGBM$table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Reference
## Prediction    A    B    C    D    E
##          A 1395    0    0    0    0
##          B    1  948    0    0    0
##          C    0    1  853    1    0
##          D    0    0    5  798    1
##          E    0    0    0    2  899&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model from the GBM method presents very high accuracy on train set (99.74%) as well as on test set (99,78%). Since these values are too close, I assume that there is no overfitting on the train set. On the other hand, one can notice that the test accuracy is slightly better than the train one. Yet, the difference is in fact too small as it corresponds to &lt;strong&gt;less than&lt;/strong&gt; two additional good predictions on the test set and could be induced by the smallest number of observations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3. Random forest&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
system.time(modelRF &amp;lt;- train(x, y, method = &amp;quot;rf&amp;quot;, trControl = control))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        User      System verstrichen 
##       58.67        1.32      446.53&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modelRF$results &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mtry  Accuracy     Kappa  AccuracySD     KappaSD
## 1    2 0.9952433 0.9939829 0.002150399 0.002720445
## 2   22 0.9986407 0.9982806 0.001749896 0.002213406
## 3   43 0.9967383 0.9958746 0.001743711 0.002205343&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predRF &amp;lt;- predict(modelRF, subTestingCor)
cfMatRF &amp;lt;- confusionMatrix(subTestingCor$classe, predRF)
rm(control)
cfMatRF$overall&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##      0.9989804      0.9987104      0.9976223      0.9996689      0.2844617 
## AccuracyPValue  McnemarPValue 
##      0.0000000            NaN&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cfMatRF$table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Reference
## Prediction    A    B    C    D    E
##          A 1395    0    0    0    0
##          B    0  948    1    0    0
##          C    0    1  854    0    0
##          D    0    0    3  801    0
##          E    0    0    0    0  901&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The obtained model shows very high accuracies. As for the GBM method, the train and test accuracies are once again too close with the latter slightly better than the first. There is thus no overfitting on the train set.&lt;/p&gt;
&lt;p&gt;To avoid doubts on the resampling method, I computed a new model using &lt;strong&gt;&lt;em&gt;repeatedcv&lt;/em&gt;&lt;/strong&gt; on the same data set. The accuracies obtained with 5 repeats (99,88%) are exactly the same and are very close to the ones obtained previously.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-between-the-different-methods&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparison between the different methods&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- resamples(list(RPART=modelRPART, GBM=modelGBM, RF=modelRF))
results$timings &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Everything FinalModel Prediction
## RPART      20.12       0.39         NA
## GBM       172.27      32.25         NA
## RF        446.53      59.48         NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bwplot(results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/weightLifting_files/figure-html/compModels-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Apart of the RPART method, the models obtained with the GBM and RF methods fit the test data very well. The standard deviations of the accuracies are also very small which suggests that the data collected with the body sensors is very accurate. The big difference in processing time between the GBM and RF methods (about the double) poses the problem of performance when using a time-consuming method for just a little improvement. Of course, for this project, one can afford using the RF method to predict the cases from the test data. However, once the data becomes very big, it is important to assess all these different aspects taking into consideration the acceptable margin of error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predictors-importance-in-random-forest&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predictor’s importance in Random Forest&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;importanceRF &amp;lt;- varImp(modelRF,scale = FALSE)
plot(importanceRF, top= 10) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/weightLifting_files/figure-html/predImportance-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is very interesting to notice that the most important predictors used in RF method are the &lt;strong&gt;&lt;em&gt;raw_timestamp_part_1&lt;/em&gt;&lt;/strong&gt; and the &lt;strong&gt;&lt;em&gt;num_window&lt;/em&gt;&lt;/strong&gt;. Since the data was recorded by body sensors using a sliding time window with different durations, it is therefore necessary to look at the observations according to their corresponding time stamp and window number. While this behavior is intuitive for a human being, the fact remains that it is not obvious for a machine. Fortunately, the RF algorithm permitted to learn from the data enough to spot this fact.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-to-the-test-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Application to the test data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(modelRF, testingCor) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 100% accuracy
#rm(list = ls())
stopCluster(cluster)
registerDoSEQ()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bibliography&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Gray, S. E., &amp;amp; Finch, C. F. (2015). The causes of injuries sustained at fitness facilities presenting to Victorian emergency departments-identifying the main culprits. Injury epidemiology, 2(1), 6.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Velloso, E., Bulling, A., Gellersen, H., Ugulino, W., &amp;amp; Fuks, H. (2013, March). Qualitative activity recognition of weight lifting exercises. In Proceedings of the 4th Augmented Human International Conference (pp. 116-123). ACM.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Pearson, K. (1896). Mathematical contributions to the theory of evolution. III. Regression, heredity, and panmixia. Philosophical Transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character, 187, 253-318.&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/&#34; class=&#34;uri&#34;&gt;http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/&lt;/a&gt;&lt;a href=&#34;#fnref4&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Impact of severe weather events in the United States</title>
      <link>https://slamara.github.io/project/severeweather/</link>
      <pubDate>Wed, 08 Aug 2018 17:08:23 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/severeweather/</guid>
      <description>&lt;div id=&#34;synopsis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Synopsis&lt;/h2&gt;
&lt;p&gt;Severe weather events have shown in the last decades an increasing threat on population safety and a costly impact on economy. Hence, their potential consequences should be studied for a better mitigation of their inherent risk. The U.S. National Oceanic and Atmospheric Administration (NOAA) has implemented a catalog where all characteristics of major storms and weather events in the United States from 1950 to November 2011 were collected. These characteristics include when and where these events occurred, as well as estimates of any fatalities, injuries, property and crop damages. The aim of the present analysis is to identify events that are most harmful to population health and economy across the United States.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data processing&lt;/h2&gt;
&lt;p&gt;Before doing such analysis, the homogeneity and completeness of the catalog should be assessed to insure reliability of the results. Unfortunately, I am forced in this study to use the catalog as it is since no further details are given.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(utils)
library(ggplot2)
library(gridExtra)
library(scales)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#download.file(&amp;quot;https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2&amp;quot;, &amp;quot;StormData.csv.bz2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StormData &amp;lt;- read.csv(&amp;quot;./severeWeather/StormData.csv.bz2&amp;quot;)
dim(StormData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 902297     37&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(StormData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;STATE__&amp;quot;    &amp;quot;BGN_DATE&amp;quot;   &amp;quot;BGN_TIME&amp;quot;   &amp;quot;TIME_ZONE&amp;quot;  &amp;quot;COUNTY&amp;quot;    
##  [6] &amp;quot;COUNTYNAME&amp;quot; &amp;quot;STATE&amp;quot;      &amp;quot;EVTYPE&amp;quot;     &amp;quot;BGN_RANGE&amp;quot;  &amp;quot;BGN_AZI&amp;quot;   
## [11] &amp;quot;BGN_LOCATI&amp;quot; &amp;quot;END_DATE&amp;quot;   &amp;quot;END_TIME&amp;quot;   &amp;quot;COUNTY_END&amp;quot; &amp;quot;COUNTYENDN&amp;quot;
## [16] &amp;quot;END_RANGE&amp;quot;  &amp;quot;END_AZI&amp;quot;    &amp;quot;END_LOCATI&amp;quot; &amp;quot;LENGTH&amp;quot;     &amp;quot;WIDTH&amp;quot;     
## [21] &amp;quot;F&amp;quot;          &amp;quot;MAG&amp;quot;        &amp;quot;FATALITIES&amp;quot; &amp;quot;INJURIES&amp;quot;   &amp;quot;PROPDMG&amp;quot;   
## [26] &amp;quot;PROPDMGEXP&amp;quot; &amp;quot;CROPDMG&amp;quot;    &amp;quot;CROPDMGEXP&amp;quot; &amp;quot;WFO&amp;quot;        &amp;quot;STATEOFFIC&amp;quot;
## [31] &amp;quot;ZONENAMES&amp;quot;  &amp;quot;LATITUDE&amp;quot;   &amp;quot;LONGITUDE&amp;quot;  &amp;quot;LATITUDE_E&amp;quot; &amp;quot;LONGITUDE_&amp;quot;
## [36] &amp;quot;REMARKS&amp;quot;    &amp;quot;REFNUM&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;correct-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correct the data&lt;/h3&gt;
&lt;p&gt;As many observations in variable EVTYPE (representing the different weather events) can be gathered in one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(StormData$EVTYPE[grep(&amp;quot;fire&amp;quot;, StormData$EVTYPE, ignore.case = TRUE)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] WILD FIRES        WILDFIRE          WILD/FOREST FIRE 
##  [4] GRASS FIRES       LIGHTNING FIRE    FOREST FIRES     
##  [7] WILDFIRES         WILD/FOREST FIRES BRUSH FIRES      
## [10] BRUSH FIRE        RED FLAG FIRE WX 
## 985 Levels:    HIGH SURF ADVISORY  COASTAL FLOOD ... WND&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The column should be updated for the events: wind, heat, fire, flood and hurricane:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elements &amp;lt;- c(&amp;quot;wind&amp;quot;, &amp;quot;heat&amp;quot;, &amp;quot;fire&amp;quot;, &amp;quot;flood&amp;quot;, &amp;quot;hurricane&amp;quot;)

StormDataCorr &amp;lt;- StormData

levels(StormDataCorr$EVTYPE) &amp;lt;- c(levels(StormDataCorr$EVTYPE), &amp;quot;FIRE&amp;quot;)
for (i in elements){
    StormDataCorr$EVTYPE[grep(i, StormDataCorr$EVTYPE, ignore.case = TRUE)] &amp;lt;- toupper(i)
}

unique(StormDataCorr$EVTYPE[grep(&amp;quot;fire&amp;quot;, StormDataCorr$EVTYPE, ignore.case = TRUE)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FIRE
## 986 Levels:    HIGH SURF ADVISORY  COASTAL FLOOD ... FIRE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-the-impact-on-population-health-fatalities-and-injuries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compute the impact on population health (fatalities and injuries)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AggreFatInj &amp;lt;- function(data = StormDataCorr, field) {
    aggre &amp;lt;- aggregate(data[,field], by = list(EVTYPE = data$EVTYPE), FUN = sum)
    aggreOrd &amp;lt;- aggre[order(-aggre$x),]
    aggreOrd &amp;lt;- aggreOrd[which(aggreOrd$x&amp;gt;0),]
    aggreOrd
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-the-economical-impact-property-and-crop-damages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compute the economical impact (property and crop damages)&lt;/h3&gt;
&lt;p&gt;Before computing the correct amount of property and crop damages, we should take in account the appropriate exponentials. First, I set up a dictionary:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exponent &amp;lt;- cbind(exp = c(&amp;quot;H&amp;quot;,&amp;quot;h&amp;quot;,&amp;quot;K&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;M&amp;quot;,&amp;quot;m&amp;quot;,&amp;quot;B&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;0&amp;quot;,&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;,&amp;quot;6&amp;quot;,&amp;quot;7&amp;quot;,&amp;quot;8&amp;quot;), 
                  value = c(10^2,10^2,10^3,10^3,10^6,10^6,10^9,10^9,1,10,10,10,10,10,10,10,10,10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then I compute the correct impact with the following function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggrePropCrop &amp;lt;- function(data = StormDataCorr, field, fieldExp) {
    
    DataDMG &amp;lt;- cbind(EVTYPE = as.character(data$EVTYPE), field = as.numeric(data[,field]) * as.numeric(exponent[match(data[,fieldExp],exponent),2]))
    
    aggreDMG &amp;lt;- aggregate(as.numeric(DataDMG[,2]), 
                          by = list(EVTYPE = DataDMG[,1]), 
                          FUN = sum, na.rm = TRUE)
    
    aggreOrd &amp;lt;- aggreDMG[order(-aggreDMG$x),]
    
    aggreOrd &amp;lt;- aggreOrd[which(aggreOrd$x&amp;gt;0),]
    
    aggreOrd
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;div id=&#34;fatalities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fatalities:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggreFatOrd &amp;lt;- AggreFatInj(StormDataCorr, &amp;quot;FATALITIES&amp;quot;)
head(aggreFatOrd, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           EVTYPE    x
## 527      TORNADO 5633
## 177         HEAT 3138
## 109        FLOOD 1525
## 607         WIND 1451
## 286    LIGHTNING  816
## 379  RIP CURRENT  368
## 14     AVALANCHE  224
## 609 WINTER STORM  206
## 380 RIP CURRENTS  204
## 102 EXTREME COLD  160&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;injuries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Injuries:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggreInjOrd &amp;lt;- AggreFatInj(StormDataCorr, &amp;quot;INJURIES&amp;quot;)
head(aggreInjOrd, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           EVTYPE     x
## 527      TORNADO 91346
## 607         WIND 11498
## 177         HEAT  9224
## 109        FLOOD  8604
## 286    LIGHTNING  5230
## 254    ICE STORM  1975
## 620         FIRE  1608
## 149         HAIL  1361
## 240    HURRICANE  1328
## 609 WINTER STORM  1321&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FatPlot &amp;lt;- ggplot(head(aggreFatOrd,10), aes(x=reorder(EVTYPE, -x), y = x)) +
    geom_bar(stat=&amp;quot;identity&amp;quot;, width = 0.5) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    theme(plot.title = element_text(hjust = 0.5)) + 
    labs(title=&amp;quot;Total fatalities by severe weather\n events in the US&amp;quot;, 
         x=&amp;quot;Severe weather elements&amp;quot;, 
         y = &amp;quot;Number of fatalities&amp;quot;)

InjPlot &amp;lt;- ggplot(head(aggreInjOrd,10), aes(x=reorder(EVTYPE, -x), y = x)) +
    geom_bar(stat=&amp;quot;identity&amp;quot;, width = 0.5) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    theme(plot.title = element_text(hjust = 0.5)) + 
    labs(title=&amp;quot;Total Injuries by severe weather\n events in the US&amp;quot;, 
         x=&amp;quot;Severe weather elements&amp;quot;, 
         y = &amp;quot;Number of Injuries&amp;quot;)

grid.arrange(FatPlot, InjPlot, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/severeWeather_files/figure-html/plot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;st-fact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1st fact:&lt;/h3&gt;
&lt;p&gt;From the tables and plots it appears clearly that &lt;strong&gt;TORNADOES&lt;/strong&gt; are the &lt;strong&gt;most harmful with respect to the population health&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;property-damage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Property damage:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggrePropOrd &amp;lt;- aggrePropCrop(StormDataCorr, &amp;quot;PROPDMG&amp;quot;, &amp;quot;PROPDMGEXP&amp;quot;)
head(aggrePropOrd, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               EVTYPE            x
## 110            FLOOD 167523218973
## 241        HURRICANE  84656180010
## 528          TORNADO  56937162837
## 448      STORM SURGE  43323536000
## 608             WIND  17742639462
## 150             HAIL  15732269877
## 106             FIRE   8501628500
## 541   TROPICAL STORM   7703890550
## 610     WINTER STORM   6688497260
## 449 STORM SURGE/TIDE   4641188000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;crop-damage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Crop damage:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggreCropOrd &amp;lt;- aggrePropCrop(StormDataCorr, &amp;quot;CROPDMG&amp;quot;, &amp;quot;CROPDMGEXP&amp;quot;)
head(aggreCropOrd, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           EVTYPE           x
## 64       DROUGHT 13972566000
## 110        FLOOD 12267259100
## 241    HURRICANE  5505292800
## 255    ICE STORM  5022113500
## 150         HAIL  3025954650
## 608         WIND  2159305250
## 102 EXTREME COLD  1292973000
## 134 FROST/FREEZE  1094086000
## 178         HEAT   904469280
## 186   HEAVY RAIN   733399800&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PropPlot &amp;lt;- ggplot(head(aggrePropOrd,10), aes(x=reorder(EVTYPE, -x), y = x)) + 
    geom_bar(stat=&amp;quot;identity&amp;quot;, width = 0.5) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size=12)) +
    theme(plot.title = element_text(hjust = 0.5)) +
    scale_y_continuous(labels = comma) + 
    labs(title=&amp;quot;Total amount of property damages\n by severe weather events in the US&amp;quot;, 
         x=&amp;quot;Severe weather elements&amp;quot;, 
         y = &amp;quot;Amount of property damages&amp;quot;)

CropPlot &amp;lt;- ggplot(head(aggreCropOrd,10), aes(x=reorder(EVTYPE, -x), y = x)) + 
    geom_bar(stat=&amp;quot;identity&amp;quot;, width = 0.5) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size=12)) +
    theme(plot.title = element_text(hjust = 0.5)) + 
    scale_y_continuous(labels = comma) + 
    labs(title=&amp;quot;Total amount of crop damages\n by severe weather events in the US&amp;quot;, 
         x=&amp;quot;Severe weather elements&amp;quot;, 
         y = &amp;quot;Amount of crop damages&amp;quot;)

grid.arrange(PropPlot, CropPlot, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/severeWeather_files/figure-html/plot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nd-fact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2nd fact&lt;/h3&gt;
&lt;p&gt;From the tables and plots, it appears clearly that &lt;strong&gt;FLOODS&lt;/strong&gt; and &lt;strong&gt;DROUGHTS&lt;/strong&gt; have the &lt;strong&gt;greatest economic consequences&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Influence of automatic and manual transmission on fuel consumption</title>
      <link>https://slamara.github.io/project/fuelconsumption/</link>
      <pubDate>Wed, 08 Aug 2018 16:53:36 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/fuelconsumption/</guid>
      <description>&lt;div id=&#34;executive-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Executive summary:&lt;/h2&gt;
&lt;p&gt;The purpose of this project is to assess qualitatively and quantitatively the leverage of automatic and manual transmission (&lt;strong&gt;am&lt;/strong&gt;) on the fuel consumption (&lt;strong&gt;mpg&lt;/strong&gt;) of a selection of 32 cars. This should be done with regards to the intrinsic relationship of &lt;strong&gt;mpg&lt;/strong&gt; and &lt;strong&gt;am&lt;/strong&gt; with 10 other car aspects and performances. The data used in this assignment was published in the 1974 Motors Trend magazine.&lt;/p&gt;
&lt;p&gt;In this short report, I start with an exploratory analysis and rapid graphical representation focusing roughly on the &lt;strong&gt;am&lt;/strong&gt; and &lt;strong&gt;mpg&lt;/strong&gt;. In the second part I examine different models and select the best one inferring the relationship between these variables. The report ends with a conclusion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset “mtcars” can be loaded with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data set consists of 11 different characteristics of 32 car models from the 70’s.&lt;/p&gt;
&lt;p&gt;The box plot computed for &lt;strong&gt;mpg&lt;/strong&gt; with regards to &lt;strong&gt;am&lt;/strong&gt; shows clearly that the manual transmission have a higher &lt;strong&gt;mpg&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(as.factor(am), mpg, data = mtcars, geom = &amp;quot;boxplot&amp;quot;, color = as.factor(am), 
      xlab = &amp;quot;Type of transmission(0: automatic, 1: manual)&amp;quot;, 
      ylab = &amp;quot;Number of miles per gallon&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/fuelConsumption_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, the correlation between &lt;strong&gt;mpg&lt;/strong&gt; and the other parameters shows a stronger relationship between &lt;strong&gt;mpg&lt;/strong&gt; and &lt;strong&gt;wt&lt;/strong&gt;, &lt;strong&gt;cyl&lt;/strong&gt;, &lt;strong&gt;disp&lt;/strong&gt;, &lt;strong&gt;hp&lt;/strong&gt;, &lt;strong&gt;drat&lt;/strong&gt;, &lt;strong&gt;vs&lt;/strong&gt; as compared to &lt;strong&gt;am&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corr &amp;lt;- cor(mtcars$mpg, mtcars)
corr[1, order(-abs(corr[1,]))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        mpg         wt        cyl       disp         hp       drat 
##  1.0000000 -0.8676594 -0.8521620 -0.8475514 -0.7761684  0.6811719 
##         vs         am       carb       gear       qsec 
##  0.6640389  0.5998324 -0.5509251  0.4802848  0.4186840&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hence, I consider that since &lt;strong&gt;mpg&lt;/strong&gt; is strongly correlated with other parameters, it could be misleading to ignore their effects on its relationship with the automatic and manual transmissions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model selection&lt;/h2&gt;
&lt;p&gt;As first look, I compute a regression model with &lt;strong&gt;mpg&lt;/strong&gt; as the outcome and &lt;strong&gt;am&lt;/strong&gt; as the regressor:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- lm(mpg ~ am, mtcars)
summary(fit1)$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Estimate Std. Error   t value     Pr(&amp;gt;|t|)
## (Intercept) 17.147368   1.124603 15.247492 1.133983e-15
## am           7.244939   1.764422  4.106127 2.850207e-04&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimate of the intercept represents the hypothetical fuel efficiency in case of automatic transmission (am = 0) while the estimate of &lt;strong&gt;am&lt;/strong&gt; represents the slope for the case of manual transmission (am = 1).&lt;/p&gt;
&lt;p&gt;Fitting all parameters of mtcars:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- lm(mpg ~ ., mtcars)
summary(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ ., data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4506 -1.6044 -0.1196  1.2193  4.6271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept) 12.30337   18.71788   0.657   0.5181  
## cyl         -0.11144    1.04502  -0.107   0.9161  
## disp         0.01334    0.01786   0.747   0.4635  
## hp          -0.02148    0.02177  -0.987   0.3350  
## drat         0.78711    1.63537   0.481   0.6353  
## wt          -3.71530    1.89441  -1.961   0.0633 .
## qsec         0.82104    0.73084   1.123   0.2739  
## vs           0.31776    2.10451   0.151   0.8814  
## am           2.52023    2.05665   1.225   0.2340  
## gear         0.65541    1.49326   0.439   0.6652  
## carb        -0.19942    0.82875  -0.241   0.8122  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.65 on 21 degrees of freedom
## Multiple R-squared:  0.869,  Adjusted R-squared:  0.8066 
## F-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The higher value of R-squared suggests a better fit for this model. However, the p-values, which represent the significance of each parameter in presence of the others, are very high. Thus, to determine statistically the best fitting model I use the &lt;strong&gt;step&lt;/strong&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best &amp;lt;- step(fit2, direction = &amp;quot;both&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(best)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ wt + qsec + am, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4811 -1.5555 -0.7257  1.4110  4.6610 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   9.6178     6.9596   1.382 0.177915    
## wt           -3.9165     0.7112  -5.507 6.95e-06 ***
## qsec          1.2259     0.2887   4.247 0.000216 ***
## am            2.9358     1.4109   2.081 0.046716 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.459 on 28 degrees of freedom
## Multiple R-squared:  0.8497, Adjusted R-squared:  0.8336 
## F-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to the step function, the best model accounts for &lt;strong&gt;am&lt;/strong&gt;, &lt;strong&gt;wt&lt;/strong&gt; and &lt;strong&gt;qsec&lt;/strong&gt;. The R-squared is in this case significant and the p-values small. The coefficient of &lt;strong&gt;am&lt;/strong&gt; shows an &lt;strong&gt;mpg&lt;/strong&gt; higher of about 2.94 miles per gallon in the case of manual transmission.&lt;/p&gt;
&lt;p&gt;The plots of the residuals Vs. fitted values, the square root of the standard residuals Vs. fitted values, the standard residuals Vs. Leverage and the QQ-plot are given by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(2, 2))
plot(best)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/fuelConsumption_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While the QQ-plot seems to be relatively acceptable, the other plots show that the assumptions of normality and linearity are close to be breached.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The final results don’t permit to be confident in concluding a better efficiency of the manual transmission. All I can say in this case is that the quantification of the difference in fuel efficiency between the automatic and manual transmission is only about 3 miles per gallon with a p confidence of 0.046. Due to the small number of observations, only the inclusion of more data can probably improve the confidence in the qualitative and quantitative findings.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Personal activity monitoring</title>
      <link>https://slamara.github.io/project/activitymonitoring/</link>
      <pubDate>Wed, 08 Aug 2018 16:40:59 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/activitymonitoring/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This project makes use of data from a personal activity monitoring device which counts the number of steps taken by an anonymous individual in 5 minutes intervals throughout the day. The &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip&#34;&gt;dataset&lt;/a&gt; was collected within a period of two months (October and November 2012).&lt;/p&gt;
&lt;p&gt;The variables included in this dataset are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;steps&lt;/strong&gt;: Number of steps taking in a 5-minute interval (missing values are coded as NA)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;date&lt;/strong&gt;: The date on which the measurement was taken in YYYY-MM-DD format&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;interval&lt;/strong&gt;: Identifier for the 5-minute interval in which measurement was taken&lt;/p&gt;
&lt;p&gt;The dataset is stored in a comma-separated-value (CSV) file with a total of 17,568 observations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-and-preprocessing-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading and preprocessing the data:&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(xtable)
library(chron)
library(lattice)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;activity &amp;lt;- read.csv(&amp;quot;./activityMonitoring/activity.csv&amp;quot;)

activity$date &amp;lt;- as.Date(activity$date, format=&amp;quot;%Y-%m-%d&amp;quot;)

str(activity)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    17568 obs. of  3 variables:
##  $ steps   : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ date    : Date, format: &amp;quot;2012-10-01&amp;quot; &amp;quot;2012-10-01&amp;quot; ...
##  $ interval: int  0 5 10 15 20 25 30 35 40 45 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-the-mean-total-number-of-steps-taken-per-day&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is the mean total number of steps taken per day?&lt;/h2&gt;
&lt;p&gt;I first calculate the total number of steps taken per day:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StepsPerDay &amp;lt;- activity %&amp;gt;% na.omit() %&amp;gt;% group_by(date) %&amp;gt;% summarise(TotSteps = sum(steps))

StepsPerDay&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 53 x 2
##    date       TotSteps
##    &amp;lt;date&amp;gt;        &amp;lt;int&amp;gt;
##  1 2012-10-02      126
##  2 2012-10-03    11352
##  3 2012-10-04    12116
##  4 2012-10-05    13294
##  5 2012-10-06    15420
##  6 2012-10-07    11015
##  7 2012-10-09    12811
##  8 2012-10-10     9900
##  9 2012-10-11    10304
## 10 2012-10-12    17382
## # ... with 43 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then, I make a histogram of the total number of steps taken each day:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(StepsPerDay$date, StepsPerDay$TotSteps, type=&amp;quot;h&amp;quot;, lwd=5, col=&amp;quot;red&amp;quot;, 
     xlab=&amp;quot;Days&amp;quot;, 
     ylab=&amp;quot;Number of steps&amp;quot;, 
     main=&amp;quot;Total number of steps taken each day&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/activityMonitoring_files/figure-html/histo1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, I calculate and report the mean and median of the total number of steps taken per day&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(StepsPerDay$TotSteps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10766.19&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median(StepsPerDay$TotSteps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10765&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-the-average-daily-activity-pattern&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is the average daily activity pattern?&lt;/h2&gt;
&lt;p&gt;To show the pattern, I make a time series plot (i.e. type = “l”) of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all days (y-axis)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MeanInterval &amp;lt;- aggregate(activity$steps, 
                          by=list(interval=activity$interval), 
                          FUN = mean, na.rm = TRUE)

plot(MeanInterval$interval, MeanInterval$x, type = &amp;quot;l&amp;quot;, col = &amp;quot;blue&amp;quot;, 
     xlab = &amp;quot;5-minute intervals&amp;quot;, 
     ylab = &amp;quot;Average nummber of steps&amp;quot;, 
     main = &amp;quot;Average number of steps averaged across all days&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/activityMonitoring_files/figure-html/timeSerie1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which 5-minute interval, on average across all the days in the dataset, contains the maximum number of steps?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MeanInterval[which.max(MeanInterval$x),]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     interval        x
## 104      835 206.1698&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;imputing-missing-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Imputing missing values&lt;/h2&gt;
&lt;p&gt;I first calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(is.na(activity$steps))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2304&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a strategy for filling in all of the missing values in the dataset, I use &lt;strong&gt;dplyr&lt;/strong&gt; to group the data according to the day and replace the NA with the average number of steps across all days of its corresponding interval (from the table &lt;strong&gt;MeanInterval&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;I create then a new dataset that is equal to the original dataset but with the missing data filled in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;activityFull &amp;lt;- activity %&amp;gt;% group_by(date) %&amp;gt;% mutate(steps = ifelse(is.na(steps), MeanInterval$x[match(interval, MeanInterval$interval)], steps))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A histogram of the total number of steps taken each day is then re-calculated as well as the mean and median total number of steps taken per day.&lt;/p&gt;
&lt;p&gt;While the new mean is exactly the same as in the first part, the median differs too slightly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StepsPerDayFull &amp;lt;- activityFull %&amp;gt;% group_by(date) %&amp;gt;% summarise(TotSteps = sum(steps))

plot(StepsPerDayFull$date, StepsPerDayFull$TotSteps, type=&amp;quot;h&amp;quot;, lwd=5, col=&amp;quot;red&amp;quot;, 
     xlab=&amp;quot;Days&amp;quot;, 
     ylab=&amp;quot;Number of steps&amp;quot;, 
     main=&amp;quot;Total number of steps taken each day obtained from a full data set&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/activityMonitoring_files/figure-html/histo2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Recap &amp;lt;- xtable(cbind(c(&amp;quot;Without \&amp;quot;NA\&amp;quot;&amp;quot;, &amp;quot;With \&amp;quot;NA\&amp;quot;&amp;quot;), c(mean(StepsPerDayFull$TotSteps), mean(StepsPerDay$TotSteps)), c(median(StepsPerDayFull$TotSteps), median(StepsPerDay$TotSteps))))

colnames(Recap) &amp;lt;- c(&amp;quot;Type of data set&amp;quot;, &amp;quot;Mean&amp;quot;, &amp;quot;Median&amp;quot;)

rownames(Recap) &amp;lt;- NULL

print(Recap, include.rownames=FALSE, type=&amp;quot;html&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;!-- html table generated in R 3.5.1 by xtable 1.8-2 package --&gt;
&lt;!-- Wed Aug 08 17:45:36 2018 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
Type of data set
&lt;/th&gt;
&lt;th&gt;
Mean
&lt;/th&gt;
&lt;th&gt;
Median
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
Without “NA”
&lt;/td&gt;
&lt;td&gt;
10766.1886792453
&lt;/td&gt;
&lt;td&gt;
10766.1886792453
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
With “NA”
&lt;/td&gt;
&lt;td&gt;
10766.1886792453
&lt;/td&gt;
&lt;td&gt;
10765
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;are-there-differences-in-activity-patterns-between-weekdays-and-weekends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Are there differences in activity patterns between weekdays and weekends?&lt;/h2&gt;
&lt;p&gt;I first create a new factor variable in the dataset with two levels “weekday” and “weekend” indicating whether a given date is a weekday or a weekend day.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;activityFull &amp;lt;- activityFull %&amp;gt;% mutate(WE = weekdays(date)) %&amp;gt;% mutate(WE = ifelse(WE == &amp;quot;Samstag&amp;quot;|WE == &amp;quot;Sonntag&amp;quot;, &amp;quot;weekend&amp;quot;, &amp;quot;weekday&amp;quot;))

# With the library &amp;quot;Chron&amp;quot;:
# activityFulltest &amp;lt;- activityFull %&amp;gt;% mutate(WE = ifelse(is.weekend(date), &amp;quot;weekend&amp;quot;, &amp;quot;weekday&amp;quot;)

table(activityFull$WE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## weekday weekend 
##   12960    4608&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I make a panel plot containing a time series plot (i.e. type = “l”) of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all weekday days or weekend days (y-axis).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MeanIntervalFull &amp;lt;- aggregate(activityFull$steps, 
                              by=list(interval=activityFull$interval, WE=activityFull$WE), 
                              FUN = mean)

xyplot(x ~ interval | as.factor(WE), data = MeanIntervalFull, type = &amp;quot;l&amp;quot;, 
       xlab = &amp;quot;Interval&amp;quot;, 
       ylab = &amp;quot;Number of steps&amp;quot;, 
       main = &amp;quot;The average number of steps across all weekday days or weekend days&amp;quot;, 
       layout = c(1,2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/activityMonitoring_files/figure-html/PanelPlot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Evolution of fine particulate matter pollution in the US</title>
      <link>https://slamara.github.io/project/airquality/</link>
      <pubDate>Wed, 08 Aug 2018 16:24:41 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/airquality/</guid>
      <description>&lt;p&gt;Among the numerous air pollutants, the fine particulate matter (PM2.5) is one of the most harmful to human health. In the US, the Environmental Protection Agency (EPA) is in charge of setting the ambient air quality standards, tracking the emissions of the pollutants into the atmosphere, and releasing a database on emissions approximatly every 3 years. This database is known as the National Emissions Inventory (NEI). For more details, see the &lt;a href=&#34;http://www.epa.gov/ttn/chief/eiinformation.html&#34;&gt;EPA National Emissions Inventory web site&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this Database, the NEI records, for each year, how many tons of PM2.5 were emitted. The data used in this project are for 1999, 2002, 2005, and 2008 and can be downloaded &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2FNEI_data.zip&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My goal within this report is to explore the National Emissions Inventory database and see what it says about fine particulate matter pollution in the United states over the 10-year period 1999–2008.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data frame with all of the PM2.5 emissions data for 1999, 2002, 2005, and 2008
NEI &amp;lt;- readRDS(&amp;quot;./airQuality/summarySCC_PM25.rds&amp;quot;)

# mapping from the SCC digit strings in the Emissions table to the actual name of
# the PM2.5 source
SCC &amp;lt;- readRDS(&amp;quot;./airQuality/Source_Classification_Code.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;have-total-emissions-from-pm2.5-decreased-in-the-united-states-from-1999-to-2008&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Have total emissions from PM2.5 decreased in the United States from 1999 to 2008?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;totalEm &amp;lt;- with(NEI, tapply(Emissions, year, sum, na.rm = TRUE))

plot(names(totalEm), totalEm, type =&amp;quot;l&amp;quot;, 
     xlab = &amp;quot;Year&amp;quot;, 
     ylab = &amp;quot;Total Pm(2.5) Emissions (tons)&amp;quot;, 
     main = &amp;quot;Evolution of the total PM2.5 Emissions over the US&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;have-total-emissions-from-pm2.5-decreased-in-the-baltimore-city-maryland-fips24510-from-1999-to-2008&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Have total emissions from PM2.5 decreased in the Baltimore City, Maryland (fips==“24510”) from 1999 to 2008?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NEIBal &amp;lt;- subset(NEI, fips == &amp;quot;24510&amp;quot;)

totalEmBal &amp;lt;- with(NEIBal, tapply(Emissions, year, sum, na.rm = TRUE))

plot(names(totalEmBal), totalEmBal, type =&amp;quot;l&amp;quot;, 
     xlab = &amp;quot;Year&amp;quot;,
     ylab = &amp;quot;Total Pm2.5 Emissions (tons)&amp;quot;,
     main = &amp;quot;Evolution of the total PM2.5 Emissions in Baltimore&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;of-the-four-types-of-sources-indicated-by-the-type-point-nonpoint-onroad-nonroad-variable-which-of-these-four-sources-have-seen-decreases-in-emissions-from-19992008-for-baltimore-city-which-have-seen-increases-in-emissions-from-19992008&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Of the four types of sources indicated by the type (point, nonpoint, onroad, nonroad) variable, which of these four sources have seen decreases in emissions from 1999–2008 for Baltimore City? Which have seen increases in emissions from 1999–2008?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;totalEmBalTypes &amp;lt;- aggregate(NEIBal$Emissions, 
                             by=list(type=NEIBal$type , year=NEIBal$year), 
                             FUN = sum)

with(totalEmBalTypes, qplot(year, x, 
                            color = type, 
                            geom= c(&amp;quot;point&amp;quot;, &amp;quot;line&amp;quot;), 
                            xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;Total PM2.5 Emissions (tons)&amp;quot;, 
                            main = &amp;quot;Evolution of total Emissions in Baltimore by type of source&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;across-the-united-states-how-have-emissions-from-coal-combustion-related-sources-changed-from-19992008&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Across the United States, how have emissions from coal combustion-related sources changed from 1999–2008?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SCCIndex &amp;lt;- as.vector(SCC$SCC[grep(&amp;quot;*comb.*coal&amp;quot;, SCC$Short.Name, ignore.case = TRUE)])

NEICoal &amp;lt;- subset(NEI, SCC %in% SCCIndex)

totalCoal &amp;lt;- with(NEICoal, tapply(Emissions, year, sum, na.rm = TRUE))

plot(names(totalCoal), totalCoal, type =&amp;quot;l&amp;quot;, 
     xlab = &amp;quot;Year&amp;quot;, 
     ylab = &amp;quot;Total Pm2.5 Emissions (tons)&amp;quot;, 
     main = &amp;quot;Total Emission from coal combustion-related sources over the US  &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-have-emissions-from-motor-vehicle-sources-changed-from-19992008-in-baltimore-city&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How have emissions from motor vehicle sources changed from 1999–2008 in Baltimore City?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NEIBal &amp;lt;- subset(NEI, fips == &amp;quot;24510&amp;quot; &amp;amp; type == &amp;quot;ON-ROAD&amp;quot;)

totalBalMotor &amp;lt;- with(NEIBal, tapply(Emissions, year, sum, na.rm = TRUE))

plot(names(totalBalMotor), totalBalMotor, type =&amp;quot;l&amp;quot;, 
     xlab = &amp;quot;Year&amp;quot;, 
     ylab = &amp;quot;Total Pm2.5 Emissions (tons)&amp;quot;, 
     main = &amp;quot;Total Emission from Motor Vehicles in Baltimore City&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-emissions-from-motor-vehicle-sources-in-baltimore-city-with-emissions-from-motor-vehicle-sources-in-los-angeles-county-california-fips06037.-which-city-has-seen-greater-changes-over-time-in-motor-vehicle-emissions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Compare emissions from motor vehicle sources in Baltimore City with emissions from motor vehicle sources in Los Angeles County, California (fips==“06037”). Which city has seen greater changes over time in motor vehicle emissions?&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NEIComp &amp;lt;- subset(NEI, fips %in% c(&amp;quot;24510&amp;quot;, &amp;quot;06037&amp;quot;) &amp;amp; type == &amp;quot;ON-ROAD&amp;quot;)

totalComp &amp;lt;- aggregate(NEIComp$Emissions, 
                       by=list(fips= NEIComp$fips, type=NEIComp$type, year=NEIComp$year), 
                       FUN = sum)

with(totalComp, qplot(year, x, color = fips, geom= c(&amp;quot;point&amp;quot;, &amp;quot;line&amp;quot;), 
                      xlab = &amp;quot;Year&amp;quot;, 
                      ylab = &amp;quot;Total PM2.5 Emissions (tons)&amp;quot;, 
                      main = &amp;quot;Evolution of total Emissions from Motor Vehicles in Baltimore city \n      (fips = 24510) and Los Angeles County (fips = 06037)&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/airQuality_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Individual household electric power consumption</title>
      <link>https://slamara.github.io/project/electricpconsumption/</link>
      <pubDate>Wed, 08 Aug 2018 16:08:18 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/electricpconsumption/</guid>
      <description>&lt;p&gt;The goal of this project is to examine how household energy usage varies over a 2-day period in February, 2007 using the base plotting system. The &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2Fhousehold_power_consumption.zip&#34;&gt;data&lt;/a&gt; used in here (in particular, the “Individual household electric power consumption Data Set”) are from the &lt;a href=&#34;http://archive.ics.uci.edu/ml/&#34;&gt;UC Irvine Machine Learning Repository&lt;/a&gt;, a popular repository for machine learning datasets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sqldf)
library(dplyr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Estimate the memory usage
paste0(&amp;quot;As the dataset has 2880 rows and 9 columns, it requires about &amp;quot;, 
           round(2880 * 9 * 8 / 2^20, 2), &amp;quot; Megabytes in memory.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;As the dataset has 2880 rows and 9 columns, it requires about 0.2 Megabytes in memory.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read the data using an SQL statement
sel_data &amp;lt;- read.csv.sql(&amp;quot;./electricPConsumption/household_power_consumption.txt&amp;quot;, 
                            sep = &amp;#39;;&amp;#39;, header = TRUE, 
                            sql=&amp;quot;select * from file where Date in (&amp;#39;1/2/2007&amp;#39;, &amp;#39;2/2/2007&amp;#39;)&amp;quot;)

paste0(&amp;quot;Used memory: &amp;quot;, format(object.size(sel_data), units = &amp;quot;Mb&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Used memory: 0.3 Mb&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a datetime column given a specific format
sel_data &amp;lt;- sel_data %&amp;gt;% mutate(datetime = as.POSIXct(paste(sel_data$Date, sel_data$Time), 
                                                          format=&amp;quot;%d/%m/%Y %H:%M:%S&amp;quot;))

# Plot the histogram
hist(sel_data$Global_active_power, col = &amp;quot;red&amp;quot;, main = &amp;quot;Global Active Power&amp;quot;, 
         xlab = &amp;quot;Global Active Power (kilowatts)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/electricPConsumption_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a png file from the histogram
png(file = &amp;quot;plot1.png&amp;quot;, width = 480, height = 480, bg = &amp;quot;transparent&amp;quot;)

hist(sel_data$Global_active_power, col = &amp;quot;red&amp;quot;, main = &amp;quot;Global Active Power&amp;quot;, 
         xlab = &amp;quot;Global Active Power (kilowatts)&amp;quot;)

dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(sel_data$datetime, sel_data$Global_active_power, type = &amp;quot;l&amp;quot;, xlab = &amp;quot;&amp;quot;, 
         ylab = &amp;quot;Global Active Power (kilowatts)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/electricPConsumption_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(2, 2))

plot(sel_data$datetime, sel_data$Global_active_power, type = &amp;quot;l&amp;quot;, xlab = &amp;quot;&amp;quot;, 
         ylab = &amp;quot;Global Active Power&amp;quot;)

plot(sel_data$datetime, sel_data$Voltage, type = &amp;quot;l&amp;quot;, xlab = &amp;quot;datetime&amp;quot;, 
         ylab = &amp;quot;Voltage&amp;quot;)

plot(sel_data$datetime, sel_data$Sub_metering_1, type = &amp;quot;l&amp;quot;, 
         ylim = range(sel_data$Sub_metering_1), xlab = &amp;quot;&amp;quot;, ylab= &amp;quot;Energy sub metering&amp;quot;)

par(new = TRUE)

plot(sel_data$datetime, sel_data$Sub_metering_2, type = &amp;quot;l&amp;quot;, 
         ylim = range(sel_data$Sub_metering_1), col = &amp;quot;red&amp;quot;, 
         xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, xlab = &amp;quot;&amp;quot;, ylab= &amp;quot;&amp;quot;)

par(new = TRUE)

plot(sel_data$datetime, sel_data$Sub_metering_3, type = &amp;quot;l&amp;quot;, 
         ylim = range(sel_data$Sub_metering_1), col = &amp;quot;blue&amp;quot;, 
         xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, xlab = &amp;quot;&amp;quot;, ylab= &amp;quot;&amp;quot;)

legend(&amp;quot;topright&amp;quot;,legend=c(&amp;quot;Sub_metering_1&amp;quot;, &amp;quot;Sub_metering_2&amp;quot;, &amp;quot;Sub_metering_3&amp;quot;), 
           bty = &amp;quot;n&amp;quot;, lty = c(1,1),col=c(&amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;))

plot(sel_data$datetime, sel_data$Global_reactive_power, type = &amp;quot;l&amp;quot;, 
         xlab = &amp;quot;datetime&amp;quot;, ylab = &amp;quot;Global_reactive_power&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://slamara.github.io/project/electricPConsumption_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting and cleaning data</title>
      <link>https://slamara.github.io/project/gcd/</link>
      <pubDate>Fri, 03 Aug 2018 18:05:56 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/gcd/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The purpose of this project is to show the ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis.&lt;/p&gt;
&lt;p&gt;The data used in this project represent data collected from the accelerometers of Samsung Galaxy S smartphones. For a full description please visit the following website:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones&#34; class=&#34;uri&#34;&gt;http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones&lt;/a&gt; &lt;sup&gt;&lt;a href=&#34;#myfootnote1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Here are the data for the project:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip&#34;&gt;https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The script does the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load the libraries &lt;em&gt;data.table&lt;/em&gt; and &lt;em&gt;dplyr&lt;/em&gt; needed to run the script&lt;/li&gt;
&lt;li&gt;Read the files &lt;em&gt;subject_test.txt&lt;/em&gt;, &lt;em&gt;y_test.txt&lt;/em&gt;, &lt;em&gt;X_test.txt&lt;/em&gt; contained in folder &lt;strong&gt;test&lt;/strong&gt; as well as the files &lt;em&gt;subject_train.txt&lt;/em&gt;, &lt;em&gt;y_train.txt&lt;/em&gt;, &lt;em&gt;X_train.txt&lt;/em&gt; in folder &lt;strong&gt;train&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Extract features and activity names to Label the data sets with descriptive variable names&lt;/li&gt;
&lt;li&gt;Merge the training and the tests sets to create one data set&lt;/li&gt;
&lt;li&gt;Coerce the column names to obtain syntactically valid ones&lt;/li&gt;
&lt;li&gt;Extract only the measurements on the mean and standard deviation for each measurement (&lt;strong&gt;sel_data&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Use descriptive activity names to name the activities in the selected data set&lt;/li&gt;
&lt;li&gt;Create a second independent tidy data set with the average of each variable for each activity and each subject (&lt;strong&gt;mean_data&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Create the output files &lt;em&gt;sel_data.csv&lt;/em&gt; and &lt;em&gt;mean_data.csv&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Re-initialize the Global Environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries

library(data.table)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read Dataset

subject_test &amp;lt;- fread(&amp;quot;./gcd/subject_test.txt&amp;quot;)
y_test &amp;lt;- fread(&amp;quot;./gcd/y_test.txt&amp;quot;)
x_test &amp;lt;- fread(&amp;quot;./gcd/X_test.txt&amp;quot;)

subject_train &amp;lt;- fread(&amp;quot;./gcd/subject_train.txt&amp;quot;)
y_train &amp;lt;- fread(&amp;quot;./gcd/y_train.txt&amp;quot;)
x_train &amp;lt;- fread(&amp;quot;./gcd/X_train.txt&amp;quot;)

# Extract features and activity names

cnames &amp;lt;- fread(&amp;quot;./gcd/features.txt&amp;quot;)

lActivities &amp;lt;- fread(&amp;quot;./gcd/activity_labels.txt&amp;quot;)

# Label the data sets with descriptive variable names

colnames(y_test) &amp;lt;- &amp;quot;activity&amp;quot;
colnames(subject_test) &amp;lt;- &amp;quot;subject&amp;quot;
colnames(x_test) &amp;lt;- as.character(cnames$V2)
all_test &amp;lt;- cbind(subject_test, y_test, x_test)

colnames(y_train) &amp;lt;- &amp;quot;activity&amp;quot;
colnames(subject_train) &amp;lt;- &amp;quot;subject&amp;quot;
colnames(x_train) &amp;lt;- as.character(cnames$V2)
all_train &amp;lt;- cbind(subject_train, y_train, x_train)


# Merge the training and the tests sets to create one data set

all_data &amp;lt;- rbind(all_test, all_train)
paste0(&amp;quot;Number of variables: &amp;quot;, dim(all_data)[2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of variables: 563&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste0(&amp;quot;Number of Observations: &amp;quot;, dim(all_data)[1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of Observations: 10299&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Coerce column names to obtain syntactically valid ones

valid_names &amp;lt;- make.names(names=names(all_data), unique=TRUE, allow_ = TRUE)
names(all_data) &amp;lt;- valid_names

# Extract only the measurements on the mean and standard deviation for each measurement

sel_data &amp;lt;- select(all_data, matches(&amp;quot;subject|activity|\\.mean\\.|\\.std\\.&amp;quot;))

names(sel_data) &amp;lt;- gsub(names(sel_data), pattern = &amp;quot;\\.\\.&amp;quot;, replacement = &amp;quot;&amp;quot;)

# Use descriptive activity names to name the activities in the selected data set

sel_data$activity &amp;lt;- lActivities$V2[match(sel_data$activity, lActivities$V1)]

paste0(&amp;quot;The recorded activities are: &amp;quot;, paste(unique(sel_data$activity), collapse = &amp;quot;, &amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;The recorded activities are: STANDING, SITTING, LAYING, WALKING, WALKING_DOWNSTAIRS, WALKING_UPSTAIRS&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.table(sel_data, file = &amp;quot;./gcd/sel_data.txt&amp;quot;, row.names = FALSE)

# From the previous data set, create a second independent tidy data set with the average
# of each variable for each activity and each subject

mean_data &amp;lt;- sel_data %&amp;gt;% group_by(subject, activity) %&amp;gt;% summarise_all(funs(mean))

write.table(mean_data, file = &amp;quot;./gcd/mean_data.txt&amp;quot;, row.names = FALSE)

# Re-initialize the Global Environment

rm(list = ls())&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;p&gt;&lt;a name=&#34;myfootnote1&#34;&gt;[1]&lt;/a&gt;: Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ranking US Hospitals</title>
      <link>https://slamara.github.io/project/rankingushospitals/</link>
      <pubDate>Fri, 03 Aug 2018 18:05:56 +0200</pubDate>
      
      <guid>https://slamara.github.io/project/rankingushospitals/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The purpose of this project is to rank over 4000 US hospitals according to the quality of care. The data represent a small subset of the data available at the Hospital Compare web site (&lt;a href=&#34;http://hospitalcompare.hhs.gov&#34; class=&#34;uri&#34;&gt;http://hospitalcompare.hhs.gov&lt;/a&gt;) run by the U.S. Department of Health and Human Services.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/rprog%2Fdata%2FProgAssignment3-data.zip&#34;&gt;zip file&lt;/a&gt; for this project contains three files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;outcome-of-care-measures.csv&lt;/strong&gt;: Contains information about 30-day mortality and readmission rates for heart attacks, heart failure, and pneumonia for over 4,000 hospitals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hospital-data.csv&lt;/strong&gt;: Contains information about each hospital.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hospital_Revised_Flatfiles.pdf&lt;/strong&gt;: Descriptions of the variables in each file (i.e the code book). This document contains information about many other files that are not included with this project. We want to focus on the variables for Number 19 (“Outcome of Care Measures.csv”) and Number 11 (“Hospital Data.csv”).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;finding-the-best-hospital-in-a-state&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Finding the best hospital in a state&lt;/h2&gt;
&lt;p&gt;I write a function called &lt;strong&gt;best&lt;/strong&gt; which takes two arguments: the 2-character abbreviated name of a state and an outcome name. The function returns a character vector with the name of the hospital that has the lowest 30-day mortality for the specified outcome in that state. The outcomes can be one of “heart attack”, “heart failure”, or “pneumonia”. The Hospitals that do not have data on a particular outcome are excluded from the set of hospitals when deciding the rankings.&lt;/p&gt;
&lt;p&gt;If there is a tie for the best hospital for a given outcome, then the hospital names should be sorted in alphabetical order and the first hospital in that set should be chosen.&lt;/p&gt;
&lt;p&gt;The function checks the validity of its arguments and throws an error via the stop function with the message “invalid state” or “invalid outcome” when an invalid state resp. outcome value is passed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best &amp;lt;- function(state, outcome) {
## Read outcome data

    data &amp;lt;- read.csv(&amp;quot;./rankingUsHospitals/outcome-of-care-measures.csv&amp;quot;, 
                         colClasses = &amp;quot;character&amp;quot;)

## Check that state and outcome are valid

    if (!(state %in% data$State)) {
        result &amp;lt;- &amp;quot;invalid state&amp;quot;
      }
    else if (!outcome %in% c(&amp;quot;heart attack&amp;quot;, &amp;quot;heart failure&amp;quot;, &amp;quot;pneumonia&amp;quot;)) {
        result &amp;lt;- &amp;quot;invalid outcome&amp;quot;
      }
    else{
        keys &amp;lt;- c(&amp;quot;heart attack&amp;quot; = 11, &amp;quot;heart failure&amp;quot; = 17, &amp;quot;pneumonia&amp;quot; = 23)
        outcomeKey &amp;lt;- keys[outcome]
  
## Return hospital name in that state with lowest 30-day death rate
  
        dataPerState &amp;lt;- split(data, data$State)
        dataOurState &amp;lt;- dataPerState[[state]]
        dataOurState &amp;lt;- dataOurState[ order(dataOurState[&amp;quot;Hospital.Name&amp;quot;]), ]
        dataOutcome &amp;lt;- suppressWarnings(as.numeric(dataOurState[, outcomeKey]))
        good &amp;lt;- complete.cases(dataOutcome)
        dataOutcome &amp;lt;- dataOutcome[good]
        dataOurState &amp;lt;- dataOurState[good,]
        minimum &amp;lt;- min(dataOutcome)
        index &amp;lt;- match(minimum, dataOutcome)
        result &amp;lt;- dataOurState[index, 2]
    }
    result
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;testing-best&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing &lt;strong&gt;best&lt;/strong&gt;:&lt;/h3&gt;
&lt;p&gt;A set of state names and outcomes is used to check the function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chk1 &amp;lt;- c(&amp;quot;TX&amp;quot;, &amp;quot;heart attack&amp;quot;)
chk2 &amp;lt;- c(&amp;quot;TX&amp;quot;, &amp;quot;heart failure&amp;quot;)
chk3 &amp;lt;- c(&amp;quot;MD&amp;quot;, &amp;quot;heart attack&amp;quot;)
chk4 &amp;lt;- c(&amp;quot;MD&amp;quot;, &amp;quot;pneumonia&amp;quot;)
chk5 &amp;lt;- c(&amp;quot;BB&amp;quot;, &amp;quot;heart attack&amp;quot;)
chk6 &amp;lt;- c(&amp;quot;NY&amp;quot;, &amp;quot;hert attack&amp;quot;)
dat &amp;lt;- data.table(chk1, chk2, chk3, chk4, chk5, chk6)
dat &amp;lt;- t(dat)
as.list(apply(dat, 1, function(x){do.call(best, as.list(x))}))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $chk1
## [1] &amp;quot;CYPRESS FAIRBANKS MEDICAL CENTER&amp;quot;
## 
## $chk2
## [1] &amp;quot;FORT DUNCAN MEDICAL CENTER&amp;quot;
## 
## $chk3
## [1] &amp;quot;JOHNS HOPKINS HOSPITAL, THE&amp;quot;
## 
## $chk4
## [1] &amp;quot;GREATER BALTIMORE MEDICAL CENTER&amp;quot;
## 
## $chk5
## [1] &amp;quot;invalid state&amp;quot;
## 
## $chk6
## [1] &amp;quot;invalid outcome&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ranking-hospitals-by-outcome-in-a-state&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ranking hospitals by outcome in a state&lt;/h2&gt;
&lt;p&gt;To this intent, I write a function &lt;strong&gt;rankHospital&lt;/strong&gt; which takes three arguments: the 2-character abbreviated name of a state (state), an outcome (outcome), and the ranking of a hospital in that state for that outcome (num).&lt;/p&gt;
&lt;p&gt;The function returns a character vector with the name of the hospital that has the ranking specified by the &lt;strong&gt;num&lt;/strong&gt; argument. The &lt;strong&gt;num&lt;/strong&gt; argument can take the values “best”, “worst”, or an integer indicating the ranking.&lt;/p&gt;
&lt;p&gt;The Hospitals that do not have data on a particular outcome are excluded from the set of hospitals when deciding the rankings. Also, If the number given by num is larger than the number of hospitals in that state, then the function returns NA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rankHospital &amp;lt;- function(state, outcome, num = &amp;quot;best&amp;quot;) {
    
  
## Read outcome data

    data &amp;lt;- read.csv(&amp;quot;./rankingUsHospitals/outcome-of-care-measures.csv&amp;quot;, 
                         colClasses = &amp;quot;character&amp;quot;)

## Check that state and outcome are valid

    if (!(state %in% data$State)) {
        result &amp;lt;- &amp;quot;invalid state&amp;quot;
    }
    else if (!outcome %in% c(&amp;quot;heart attack&amp;quot;, &amp;quot;heart failure&amp;quot;, &amp;quot;pneumonia&amp;quot;)) {
        result &amp;lt;- &amp;quot;invalid outcome&amp;quot;
    }
    else {
        keys &amp;lt;- c(&amp;quot;heart attack&amp;quot; = 11, &amp;quot;heart failure&amp;quot; = 17, &amp;quot;pneumonia&amp;quot; = 23)
        outcomeKey &amp;lt;- keys[outcome]
  
  
## Return hospital name in that state with the given rank
## 30-day death rate
  
        dataPerState &amp;lt;- split(data, data$State)
        dataOurState &amp;lt;- dataPerState[[state]]
        dataOutcome &amp;lt;- suppressWarnings(as.numeric(dataOurState[, outcomeKey]))
        good &amp;lt;- complete.cases(dataOutcome)
        dataOutcome &amp;lt;- dataOutcome[good]
        dataOurState &amp;lt;- dataOurState[good,]
        dataOurState &amp;lt;- dataOurState[order(dataOutcome, dataOurState[&amp;quot;Hospital.Name&amp;quot;]),]
        if (grepl(&amp;quot;^[0-9]+$&amp;quot;, num)) {
            if (as.numeric(num) &amp;gt; length(dataOutcome)) {
                result &amp;lt;- NA
            }
            else {
                result &amp;lt;- dataOurState[as.numeric(num), &amp;quot;Hospital.Name&amp;quot;]
            }
        }    
        else if (num == &amp;quot;best&amp;quot;) {
                result &amp;lt;- dataOurState[1, &amp;quot;Hospital.Name&amp;quot;]
        }
        else if (num == &amp;quot;worst&amp;quot;) {
                result &amp;lt;- dataOurState[length(dataOutcome), &amp;quot;Hospital.Name&amp;quot;]
        }
        else result &amp;lt;- NA
    }
    result
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;testing-rankhospital&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing &lt;strong&gt;rankHospital&lt;/strong&gt;&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chk1 &amp;lt;- c(&amp;quot;TX&amp;quot;, &amp;quot;heart failure&amp;quot;, 4)
chk2 &amp;lt;- c(&amp;quot;MD&amp;quot;, &amp;quot;heart attack&amp;quot;, &amp;quot;worst&amp;quot;)
chk3 &amp;lt;- c(&amp;quot;MN&amp;quot;, &amp;quot;heart attack&amp;quot;, 5000)
dat &amp;lt;- data.table(chk1, chk2, chk3)
dat &amp;lt;- t(dat)
as.list(apply(dat, 1, function(x){do.call(rankHospital, as.list(x))}))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $chk1
## [1] &amp;quot;DETAR HOSPITAL NAVARRO&amp;quot;
## 
## $chk2
## [1] &amp;quot;HARFORD MEMORIAL HOSPITAL&amp;quot;
## 
## $chk3
## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ranking-hospitals-in-all-states&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ranking hospitals in all states&lt;/h2&gt;
&lt;p&gt;I implement a function &lt;strong&gt;rankAll&lt;/strong&gt; which takes as arguments the outcome name (outcome) and hospital ranking (num) and returns a 2-column data frame containing the hospital in each state that has the ranking specified in num.&lt;/p&gt;
&lt;p&gt;The function returns a value for every state (some may be NA). The first column in the data frame contains the hospital name and the second one contains the 2-character abbreviation for the state name. Hospitals that do not have data on a particular outcome are excluded from the set of hospitals when deciding the rankings.&lt;/p&gt;
&lt;p&gt;Although it is possible to call the &lt;strong&gt;rankHospital&lt;/strong&gt; function from the previous section, I decided, for didactic purposes, not using it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rankAll &amp;lt;- function(outcome, num = &amp;quot;best&amp;quot;) {

    dataAll &amp;lt;- data.frame(hospital = character(), state = character())
  
## Read outcome data

    data &amp;lt;- read.csv(&amp;quot;./rankingUsHospitals/outcome-of-care-measures.csv&amp;quot;, 
                         colClasses = &amp;quot;character&amp;quot;)
  
## Check that outcome and num are valid

    if (!outcome %in% c(&amp;quot;heart attack&amp;quot;, &amp;quot;heart failure&amp;quot;, &amp;quot;pneumonia&amp;quot;)) {
        dataAll &amp;lt;- &amp;quot;invalid outcome&amp;quot;
    }
    else {
        keys &amp;lt;- c(&amp;quot;heart attack&amp;quot; = 11, &amp;quot;heart failure&amp;quot; = 17, &amp;quot;pneumonia&amp;quot; = 23)
        outcomeKey &amp;lt;- keys[outcome]

## For each state, find the hospital of the given rank

        dataPerState &amp;lt;- split(data, data$State)
        for (stat in names(dataPerState)) {
        dataOurState &amp;lt;- dataPerState[[stat]]
        dataOutcome &amp;lt;- suppressWarnings(as.numeric(dataOurState[, outcomeKey]))
        good &amp;lt;- complete.cases(dataOutcome)
        dataOutcome &amp;lt;- dataOutcome[good]
        dataOurState &amp;lt;- dataOurState[good,]
        dataOurState &amp;lt;- dataOurState[ order(dataOutcome, dataOurState[&amp;quot;Hospital.Name&amp;quot;]), ]
        
        if (num == &amp;quot;best&amp;quot;) {
            numState &amp;lt;- c(1)
        } else {
            if (num == &amp;quot;worst&amp;quot;) {
                numState &amp;lt;- length(dataOutcome)
            } else {
                numState &amp;lt;- num
            }
        }
    
        dataPart &amp;lt;- data.frame(hospital = dataOurState[numState, &amp;quot;Hospital.Name&amp;quot;], 
                                   state = stat, row.names = stat)
        
        dataAll &amp;lt;- rbind(dataAll, dataPart)
        }
    }

## Return a data frame with the hospital names and the (abbreviated) state name

    dataAll
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;testing-rankall&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing &lt;strong&gt;rankAll&lt;/strong&gt;&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(rankAll(&amp;quot;heart attack&amp;quot;, 20), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                               hospital state
## AK                                &amp;lt;NA&amp;gt;    AK
## AL      D W MCMILLAN MEMORIAL HOSPITAL    AL
## AR   ARKANSAS METHODIST MEDICAL CENTER    AR
## AZ JOHN C LINCOLN DEER VALLEY HOSPITAL    AZ
## CA               SHERMAN OAKS HOSPITAL    CA
## CO            SKY RIDGE MEDICAL CENTER    CO
## CT             MIDSTATE MEDICAL CENTER    CT
## DC                                &amp;lt;NA&amp;gt;    DC
## DE                                &amp;lt;NA&amp;gt;    DE
## FL      SOUTH FLORIDA BAPTIST HOSPITAL    FL&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(rankAll(&amp;quot;pneumonia&amp;quot;, &amp;quot;worst&amp;quot;), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                      hospital state
## WI MAYO CLINIC HEALTH SYSTEM - NORTHLAND, INC    WI
## WV                     PLATEAU MEDICAL CENTER    WV
## WY           NORTH BIG HORN HOSPITAL DISTRICT    WY&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(rankAll(&amp;quot;heart failure&amp;quot;), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                             hospital state
## TN                         WELLMONT HAWKINS COUNTY MEMORIAL HOSPITAL    TN
## TX                                        FORT DUNCAN MEDICAL CENTER    TX
## UT VA SALT LAKE CITY HEALTHCARE - GEORGE E. WAHLEN VA MEDICAL CENTER    UT
## VA                                          SENTARA POTOMAC HOSPITAL    VA
## VI                            GOV JUAN F LUIS HOSPITAL &amp;amp; MEDICAL CTR    VI
## VT                                              SPRINGFIELD HOSPITAL    VT
## WA                                         HARBORVIEW MEDICAL CENTER    WA
## WI                                    AURORA ST LUKES MEDICAL CENTER    WI
## WV                                         FAIRMONT GENERAL HOSPITAL    WV
## WY                                        CHEYENNE VA MEDICAL CENTER    WY&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>https://slamara.github.io/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>https://slamara.github.io/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
